{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP7Zob0hLEdluw7pt628Tyu"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"2gbeEI7muUPi","executionInfo":{"status":"ok","timestamp":1702675975368,"user_tz":300,"elapsed":6,"user":{"displayName":"Jiarui Tang","userId":"07730846287814974938"}}},"outputs":[],"source":["# This notebook was adapted from official PyTorch Tutorial\n","%matplotlib inline"]},{"cell_type":"markdown","source":["Mount Google Drive first:"],"metadata":{"id":"DR4kngewgrKj"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GOgYG6n7gvJE","executionInfo":{"status":"ok","timestamp":1702676002719,"user_tz":300,"elapsed":27356,"user":{"displayName":"Jiarui Tang","userId":"07730846287814974938"}},"outputId":"4730defd-c572-4de8-c5a6-6cf2da860a99"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["# A Gentle Introduction to ``torch.autograd``\n","\n","``torch.autograd`` is PyTorchâ€™s automatic differentiation engine that powers\n","neural network training. In this section, you will get a conceptual\n","understanding of how autograd helps a neural network train.\n","\n","## Background\n","Neural networks (NNs) are a collection of nested functions that are\n","executed on some input data. These functions are defined by *parameters*\n","(consisting of weights and biases), which in PyTorch are stored in\n","tensors.\n","\n","Training a NN happens in two steps:\n","\n","**Forward Propagation**: In forward prop, the NN makes its best guess\n","about the correct output. It runs the input data through each of its\n","functions to make this guess.\n","\n","**Backward Propagation**: In backprop, the NN adjusts its parameters\n","proportionate to the error in its guess. It does this by traversing\n","backwards from the output, collecting the derivatives of the error with\n","respect to the parameters of the functions (*gradients*), and optimizing\n","the parameters using gradient descent.\n","\n","## Usage in PyTorch\n","Let's take a look at a single training step of a linear regression model.\n","\n","In this example, We'll create a dataset, define a linear model, and then use Autograd to compute gradients for optimizing our model.\n","\n","<div class=\"alert alert-info\"><h4>Note</h4><p>This tutorial works only on the CPU and will not work on GPU devices (even if tensors are moved to CUDA).</p></div>"],"metadata":{"id":"WB6o0NM5ugDX"}},{"cell_type":"markdown","source":["First, we need to import necessary libraries and create a simple dataset."],"metadata":{"id":"MFUuup-BxId_"}},{"cell_type":"code","source":["## Import Necessary Libraries:\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","## Create a Simple Dataset: For simplicity, we use a small dataset with a linear relationship\n","# Features (inputs)\n","x = torch.tensor([[1.0], [2.0], [3.0], [4.0]], requires_grad=True)\n","# Targets/Labels (outputs)\n","y = torch.tensor([[2.0], [4.0], [6.0], [8.0]], requires_grad=False)"],"metadata":{"id":"usC0IW3euviI","executionInfo":{"status":"ok","timestamp":1702689587164,"user_tz":300,"elapsed":491,"user":{"displayName":"Jiarui Tang","userId":"07730846287814974938"}}},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":["You may have noticed that we have one more argument **``requires_grad=True``** or **``requires_grad=False``** when we create the two tensors. The ``requires_grad`` attribute in PyTorch is a flag for each tensor that determines whether or not gradients should be computed for operations involving that tensor. When ``requires_grad=True`` is set for a tensor in PyTorch, it indicates that PyTorch should track all operations performed on the tensor and compute gradients with respect to that tensor during the backward pass. Reversely, when you set ``requires_grad=False``, you're telling PyTorch that you don't need gradients for that particular tensor."],"metadata":{"id":"24Fux9vKzJyk"}},{"cell_type":"markdown","source":["Next, we are going to define our simple linear model using ``nn.Linear``."],"metadata":{"id":"BjWrTDVywvUa"}},{"cell_type":"code","source":["## Define a Linear Model: We'll use a single linear layer as our model\n","model = nn.Linear(in_features=1, out_features=1)"],"metadata":{"id":"7vETqgGpxEDs","executionInfo":{"status":"ok","timestamp":1702676007831,"user_tz":300,"elapsed":7,"user":{"displayName":"Jiarui Tang","userId":"07730846287814974938"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["We'll define a loss function and optimizer:\n","- A common choice for linear regression is the Mean Squared Error (MSE) loss.\n","- We'll use Stochastic Gradient Descent (SGD) as our optimizer, with a learning rate (lr) of 0.01."],"metadata":{"id":"dDRHOZe2xTWy"}},{"cell_type":"code","source":["# define loss function\n","criterion = nn.MSELoss()\n","\n","# define optimizer\n","optimizer = optim.SGD(model.parameters(), lr=0.01)"],"metadata":{"id":"dSmODiBqxeGx","executionInfo":{"status":"ok","timestamp":1702676009775,"user_tz":300,"elapsed":1949,"user":{"displayName":"Jiarui Tang","userId":"07730846287814974938"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["Up to now, we have defined all we need for our model. We can run the input data through the model through each of its layers (only one linear layer here) to make a prediction. This is the **forward pass**."],"metadata":{"id":"W8sTcMJqxpsO"}},{"cell_type":"code","source":["y_pred = model(x)"],"metadata":{"id":"DJIJViCjySeV","executionInfo":{"status":"ok","timestamp":1702676009775,"user_tz":300,"elapsed":34,"user":{"displayName":"Jiarui Tang","userId":"07730846287814974938"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["This is an important step of understanding Autograd in PyTorch. Here, we use the model's prediction and the corresponding label to calculate the error, which is the MSE loss as defined above (``loss``).\n","Then, we are going to backpropagate this error through the network.\n","Backward propagation is kicked off when we call **``.backward()``** on the error tensor.\n","Autograd then calculates and **stores the gradients for each model parameter in the parameter's ``.grad`` attribute.**"],"metadata":{"id":"BxyrJ_Bmyifl"}},{"cell_type":"code","source":["# Compute the loss\n","loss = criterion(y_pred, y)\n","loss.backward()"],"metadata":{"id":"ulRynQZTyYoa","executionInfo":{"status":"ok","timestamp":1702676009775,"user_tz":300,"elapsed":33,"user":{"displayName":"Jiarui Tang","userId":"07730846287814974938"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["Below, we will take a look at the gradients of each parameter."],"metadata":{"id":"zbzFLib64ftZ"}},{"cell_type":"code","source":["for param in model.parameters():\n","  print(param.grad)"],"metadata":{"id":"QVOtVZRJ4e4s","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702676009776,"user_tz":300,"elapsed":33,"user":{"displayName":"Jiarui Tang","userId":"07730846287814974938"}},"outputId":"2c09b994-5264-4df3-cab9-c5221160b656"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[-28.8808]])\n","tensor([-9.7734])\n"]}]},{"cell_type":"markdown","source":["Finally, we call ``.step()`` to initiate gradient descent. The optimizer adjusts each parameter by its gradient stored in ``.grad``."],"metadata":{"id":"eJg4DgZu2B6D"}},{"cell_type":"code","source":["optimizer.step() #gradient descent"],"metadata":{"id":"lors91TE2MOe","executionInfo":{"status":"ok","timestamp":1702676009776,"user_tz":300,"elapsed":31,"user":{"displayName":"Jiarui Tang","userId":"07730846287814974938"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["At this point, we have everything needed to train your neural network.\n","\n","Next, let's train the model for 100 epochs. Take a close look at the comments to understand each step."],"metadata":{"id":"QaE0f1Yh4wnS"}},{"cell_type":"code","source":["# Number of epochs (iterations over the dataset)\n","epochs = 100\n","for epoch in range(epochs):\n","    # Forward pass: Compute predicted y by passing x to the model\n","    y_pred = model(x)\n","\n","    # Compute the loss\n","    loss = criterion(y_pred, y)\n","\n","    # Zero gradients, perform a backward pass, and update the weights\n","    optimizer.zero_grad()  # Clear the gradients of all optimized tensors\n","    loss.backward()        # Backpropagation to compute gradients\n","    optimizer.step()       # Update model parameters\n","\n","    # Optional: Print loss every 10 epochs\n","    if (epoch+1) % 10 == 0:\n","        print(f'Epoch {epoch+1}/{epochs}, Loss: {loss.item()}')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-1BUzsig2ZGw","executionInfo":{"status":"ok","timestamp":1702676009776,"user_tz":300,"elapsed":30,"user":{"displayName":"Jiarui Tang","userId":"07730846287814974938"}},"outputId":"661c1a35-7a9c-45cb-ea85-4d21f96537c4"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 10/100, Loss: 0.7235443592071533\n","Epoch 20/100, Loss: 0.02189593017101288\n","Epoch 30/100, Loss: 0.003557650838047266\n","Epoch 40/100, Loss: 0.002909100614488125\n","Epoch 50/100, Loss: 0.0027283576782792807\n","Epoch 60/100, Loss: 0.00256925355643034\n","Epoch 70/100, Loss: 0.0024196926970034838\n","Epoch 80/100, Loss: 0.0022788625210523605\n","Epoch 90/100, Loss: 0.002146214945241809\n","Epoch 100/100, Loss: 0.002021300606429577\n"]}]},{"cell_type":"markdown","source":["After training, you can test the model with a new input."],"metadata":{"id":"o9DAHDC95qCN"}},{"cell_type":"code","source":["test_x = torch.tensor([[5.0]])\n","predicted = model(test_x)\n","print(f\"Predicted value for input {test_x.item()}: {predicted.item()}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lDUna_hp3L5a","executionInfo":{"status":"ok","timestamp":1702676009776,"user_tz":300,"elapsed":22,"user":{"displayName":"Jiarui Tang","userId":"07730846287814974938"}},"outputId":"65995248-399e-4ea3-f369-cfbb28aa430e"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Predicted value for input 5.0: 9.923155784606934\n"]}]},{"cell_type":"markdown","source":["In this example, Autograd is used implicitly by PyTorch when ``loss.backward()`` is called. This computes the gradients of the loss with respect to all parameters in the model (which have ``requires_grad=True`` by default in ``nn.Linear``), which are then used by the optimizer to update these parameters."],"metadata":{"id":"vqdR5pZ559Fp"}},{"cell_type":"markdown","metadata":{"id":"xaaB5Aquuh68"},"source":["## Differentiation in Autograd\n","\n","Ref: https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html\n","\n","Let's take a look at how ``autograd`` collects gradients. We create two tensors ``a`` and ``b`` with\n","``requires_grad=True``. This signals to ``autograd`` that every operation on them should be tracked.\n","\n","\n"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"O7M3Xcv4uh69","executionInfo":{"status":"ok","timestamp":1702676009777,"user_tz":300,"elapsed":15,"user":{"displayName":"Jiarui Tang","userId":"07730846287814974938"}}},"outputs":[],"source":["import torch\n","\n","a = torch.tensor([2., 3.], requires_grad=True)\n","b = torch.tensor([6., 4.], requires_grad=True)"]},{"cell_type":"markdown","metadata":{"id":"dRMUGNaQuh69"},"source":["We create another tensor ``Q`` from ``a`` and ``b``.\n","\n","\\begin{align}Q = 3a^3 - b^2\\end{align}\n","\n"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"xy4rRF5Ouh6-","executionInfo":{"status":"ok","timestamp":1702676009777,"user_tz":300,"elapsed":14,"user":{"displayName":"Jiarui Tang","userId":"07730846287814974938"}}},"outputs":[],"source":["Q = 3*a**3 - b**2"]},{"cell_type":"markdown","metadata":{"id":"DEFYzRUyuh6-"},"source":["Let's assume ``a`` and ``b`` to be parameters of an NN, and ``Q``\n","to be the error. In NN training, we want gradients of the error\n","w.r.t. parameters, i.e.\n","\n","\\begin{align}\\frac{\\partial Q}{\\partial a} = 9a^2\\end{align}\n","\n","\\begin{align}\\frac{\\partial Q}{\\partial b} = -2b\\end{align}\n","\n","\n","When we call ``.backward()`` on ``Q``, autograd calculates these gradients\n","and stores them in the respective tensors' ``.grad`` attribute.\n","\n","We need to explicitly pass a ``gradient`` argument in ``Q.backward()`` because it is a vector.\n","``gradient`` is a tensor of the same shape as ``Q``, and it represents the\n","gradient of Q w.r.t. itself, i.e.\n","\n","\\begin{align}\\frac{dQ}{dQ} = 1\\end{align}\n","\n","Equivalently, we can also aggregate Q into a scalar and call backward implicitly, like ``Q.sum().backward()``.\n","\n","\n"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"a7B8N1Mjuh6_","executionInfo":{"status":"ok","timestamp":1702676009777,"user_tz":300,"elapsed":14,"user":{"displayName":"Jiarui Tang","userId":"07730846287814974938"}}},"outputs":[],"source":["external_grad = torch.tensor([1., 1.])\n","Q.backward(gradient=external_grad)"]},{"cell_type":"markdown","metadata":{"id":"A6MD5Kk6uh6_"},"source":["Gradients are now deposited in ``a.grad`` and ``b.grad``\n","\n"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"MglTSf7juh6_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702676009777,"user_tz":300,"elapsed":13,"user":{"displayName":"Jiarui Tang","userId":"07730846287814974938"}},"outputId":"d114674a-9aaf-4b41-9623-6ace1f7139d8"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([True, True])\n","tensor([True, True])\n"]}],"source":["# check if collected gradients are correct\n","print(9*a**2 == a.grad)\n","print(-2*b == b.grad)"]},{"cell_type":"markdown","source":["## Autograd Example in Lecture\n","\n","#### Example 1. Basic Gradient Computation\n","\n","In this example, we'll compute the gradient of a simple function with respect to its input."],"metadata":{"id":"Xt4K2YBl_OjE"}},{"cell_type":"code","source":["# Create a tensor and set requires_grad=True to track computation with it\n","x = torch.tensor(2.0, requires_grad=True)\n","\n","# Define a simple function y = x^2\n","y = x ** 2\n","\n","# Compute gradients\n","y.backward()\n","\n","# Print the gradient dy/dx\n","print(x.grad)  # Output: tensor(4.0)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jWAI3Mjt6AuU","executionInfo":{"status":"ok","timestamp":1702676010191,"user_tz":300,"elapsed":421,"user":{"displayName":"Jiarui Tang","userId":"07730846287814974938"}},"outputId":"34b05c45-a5a3-4583-e132-c0acf5757e16"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(4.)\n"]}]},{"cell_type":"markdown","source":["Here, `y` is defined as $y=x^2$. The gradient computed is $\\frac{dy}{dx}$ at $x=2$ is $2x$, which is 4.0"],"metadata":{"id":"WFx1Dmm1_2iH"}},{"cell_type":"markdown","source":["#### Example 2. Partial Derivatives\n","\n","This example shows computing partial derivatives with respect to multiple inputs using ``torch.autograd`` triggered by ``.backward()`` call."],"metadata":{"id":"8BYrxM8-Ax8K"}},{"cell_type":"code","source":["# Multiple inputs\n","x = torch.tensor(3.0, requires_grad=True)\n","w = torch.tensor(6.0, requires_grad=True)\n","b = torch.tensor(9.0, requires_grad=True)\n","\n","# Define a function\n","y = x * w + b # y = 6 * x + 9\n","\n","# Compute gradients\n","y.backward()\n","\n","# Gradients with respect to x, w, and b\n","print(x.grad)  # Output: tensor(6.0)\n","print(w.grad)  # Output: tensor(3.0)\n","print(b.grad)  # Output: tensor(1.0)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GMVZfNqQAJqR","executionInfo":{"status":"ok","timestamp":1702676010191,"user_tz":300,"elapsed":42,"user":{"displayName":"Jiarui Tang","userId":"07730846287814974938"}},"outputId":"566e27cc-7d70-4c33-8cef-83c8c86abce9"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(6.)\n","tensor(3.)\n","tensor(1.)\n"]}]},{"cell_type":"markdown","source":["#### Example 3. Gradient with Respect to a Vector\n","\n","Ref: https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html\n","\n","Consider the simplest one-layer neural network, with input ``x``, parameters ``w`` and ``b``, and some loss function. It can be defined in PyTorch in the following manner:"],"metadata":{"id":"HKR_9AK8d0Xg"}},{"cell_type":"code","source":["x = torch.ones(5,3)  # input tensor\n","y = torch.zeros(5)  # expected output\n","w = torch.randn(3, requires_grad=True)\n","b = torch.randn(5, requires_grad=True)\n","z = torch.matmul(x, w)+b\n","loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)"],"metadata":{"id":"drUS-N8meF7X","executionInfo":{"status":"ok","timestamp":1702676010192,"user_tz":300,"elapsed":33,"user":{"displayName":"Jiarui Tang","userId":"07730846287814974938"}}},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":["This code defines the following **computational graph**:\n","\n","\n","![comp-graph.png](data:image/png;base64,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)\n","\n","In this network, ``w`` and ``b`` are **parameters**, which we need to\n","optimize. Thus, we need to be able to compute the gradients of loss\n","function with respect to those variables. In order to do that, we set\n","the ``requires_grad`` property of those tensors.\n","\n","To optimize weights of parameters in the neural network, we need to\n","compute the derivatives of our loss function with respect to parameters,\n","namely, we need $\\frac{\\partial loss}{\\partial w}$ and\n","$\\frac{\\partial loss}{\\partial b}$ under some fixed values of\n","``x`` and ``y``. To compute those derivatives, we call\n","``loss.backward()``, and then retrieve the values from ``w.grad`` and\n","``b.grad``:"],"metadata":{"id":"kFJ0ojKKebsB"}},{"cell_type":"code","source":["loss.backward()\n","print(w.grad)\n","print(b.grad)"],"metadata":{"id":"Yoov92hUechd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702676010193,"user_tz":300,"elapsed":32,"user":{"displayName":"Jiarui Tang","userId":"07730846287814974938"}},"outputId":"4e97b599-46f1-4e73-f419-303a9ee62942"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([0.9321, 0.9321, 0.9321])\n","tensor([0.1947, 0.1952, 0.1665, 0.1869, 0.1887])\n"]}]},{"cell_type":"markdown","source":["#### Example 4. No Gradient for Non-Scalar Outputs\n","\n","When the output is not scalar, PyTorch requires a gradient argument in the ``backward`` method, specifying the gradients of the output with respect to some scalar function. For example, if you are going to use ``z`` in previous example as the root of backword pass, e.g.``z.backward``, a gradient argument must be specified within the parentheses of ``backward``. Otherwise, an error will be evoked."],"metadata":{"id":"zry0LPxuqoRv"}},{"cell_type":"code","source":["x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n","y = x ** 2\n","\n","# Gradient argument - same shape as y\n","gradient = torch.tensor([1.0, 1.0, 1.0])\n","y.backward(gradient)\n","\n","print(x.grad)  # Output: tensor([2.0, 4.0, 6.0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Kl5pPvRiqxJY","executionInfo":{"status":"ok","timestamp":1702676010193,"user_tz":300,"elapsed":22,"user":{"displayName":"Jiarui Tang","userId":"07730846287814974938"}},"outputId":"33bfb6f9-4252-408e-d794-d1ac03e4a0cc"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([2., 4., 6.])\n"]}]},{"cell_type":"markdown","source":["# Introduction to Optimizers"],"metadata":{"id":"nnGSge8wx4Ow"}},{"cell_type":"markdown","source":["#### Common Setup\n","\n","In this step, we define the parameters which the optimizer is going to update during the training process.\n","\n","In addition, we set up a learning rate of 0.001, a crucial hyperparameter that determines the step size at each iteration while moving towards a minimum of the loss function."],"metadata":{"id":"zUalR6J2x-Yh"}},{"cell_type":"code","source":["import torch.optim as optim\n","params = torch.tensor([1.0, 0.0], requires_grad=True)\n","learning_rate = 1e-3"],"metadata":{"id":"eMyG6LwSsAUT","executionInfo":{"status":"ok","timestamp":1702676010194,"user_tz":300,"elapsed":13,"user":{"displayName":"Jiarui Tang","userId":"07730846287814974938"}}},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":["Remember to set ``requires_grad=True``, requiring gradients for optimization."],"metadata":{"id":"LtoMSvLJ1fI2"}},{"cell_type":"markdown","source":["#### SGD Optimizer\n","\n"],"metadata":{"id":"gUXpumhX1Oy_"}},{"cell_type":"code","source":["optimizer = optim.SGD([params], lr=learning_rate)"],"metadata":{"id":"Xj5nG5_41KPm","executionInfo":{"status":"ok","timestamp":1702676010194,"user_tz":300,"elapsed":12,"user":{"displayName":"Jiarui Tang","userId":"07730846287814974938"}}},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":["#### Adam Optimizer\n","\n"],"metadata":{"id":"j2Ynjk-M1y1L"}},{"cell_type":"code","source":["optimizer = optim.Adam([params], lr=learning_rate)"],"metadata":{"executionInfo":{"status":"ok","timestamp":1702676010195,"user_tz":300,"elapsed":13,"user":{"displayName":"Jiarui Tang","userId":"07730846287814974938"}},"id":"IASX_F-a1y1N"},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":["#### Updating Parameters"],"metadata":{"id":"uOs9rKOk1-KY"}},{"cell_type":"code","source":["optimizer.step()  # Updates 'params' based on gradients and learning rate"],"metadata":{"id":"XxTbPFQ51vxG","executionInfo":{"status":"ok","timestamp":1702676010195,"user_tz":300,"elapsed":12,"user":{"displayName":"Jiarui Tang","userId":"07730846287814974938"}}},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":["The **``step()``** method is a key function that replaces manual parameter updates."],"metadata":{"id":"ewAUllZp2C_y"}},{"cell_type":"markdown","source":["# Introduction to Neural Network Module\n","\n","In PyTorch the **``torch.nn``** package defines a set of modules which are similar to the layers of a neural network. A module receives input tensors and computes output tensors. The **``torch.nn``** package also defines a set of useful loss functions that are commonly used when training neural networks. This notebook serves as an introduction to the torch.nn module in PyTorch, which provides the necessary tools for building neural networks.\n","\n","Before we begin, let's import the necessary PyTorch modules."],"metadata":{"id":"kz3bxu8dTTmy"}},{"cell_type":"code","source":["import torch.nn as nn"],"metadata":{"id":"l83p9_9DWKZ2","executionInfo":{"status":"ok","timestamp":1702676051456,"user_tz":300,"elapsed":444,"user":{"displayName":"Jiarui Tang","userId":"07730846287814974938"}}},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":["\n","## Key Components\n","\n","There are several key components in the **``torch.nn``** package:\n","- Layers: Includes various types like Linear (fully connected), Convolutional, and Recurrent layers.\n","- Activation Functions: Essential non-linearities like ReLU, Sigmoid, and Tanh.\n","- Loss Functions: Crucial for training, such as Cross-Entropy and Mean Squared Error (MSE).\n","- Utilities: For tasks like weight initialization and model parameter management.\n","\n","Each component plays a specific role in the architecture and functionality of a neural network model.\n","\n","This section introduces each of the key components of torch.nn, demonstrating how to use them in building a neural network.\n","\n","### 1. Layers\n","\n","Neural network layers are the building blocks of any model. Here we look at some commonly used layers in **``torch.nn``**.\n","\n","#### Linear (Fully Connected) Layer\n","\n","A linear layer, also known as a fully connected layer, is one of the simplest and most commonly used layers."],"metadata":{"id":"NAmR-3BWTnfT"}},{"cell_type":"code","source":["# Creating a linear layer with 100 input features and 50 output features\n","linear_layer = nn.Linear(in_features=100, out_features=50)"],"metadata":{"id":"8xdnL9CrTXRq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Convolutional Layer\n","\n","Convolutional layers are the cornerstone of Convolutional Neural Networks (CNNs), primarily used for image processing."],"metadata":{"id":"J0Da_CX4Wi6U"}},{"cell_type":"code","source":["# Convolutional layer for image processing, with 3 input channels, 32 output channels, and a kernel size of 5\n","conv_layer = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=5)"],"metadata":{"id":"uUw3HwIdWov3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Recurrent Layer (LSTM)\n","\n","LSTM layers are a type of recurrent neural network layer used for processing sequences and time series data."],"metadata":{"id":"E68sTK1cW2Gz"}},{"cell_type":"code","source":["# LSTM layer for sequence processing, with input size 10 and hidden size 20\n","lstm_layer = nn.LSTM(input_size=10, hidden_size=20)"],"metadata":{"id":"P-SjYItjW5Vm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 2. Activation Functions\n","\n","Activation functions introduce non-linear properties to the network, enabling them to learn more complex data patterns."],"metadata":{"id":"5ZiUOQh4W8Ju"}},{"cell_type":"markdown","source":["#### ReLU (Rectified Linear Unit)\n","\n","ReLU (Rectified Linear Unit) is a widely used activation function, especially in CNNs.\n"],"metadata":{"id":"jUxBoyNjW_5X"}},{"cell_type":"code","source":["relu = nn.ReLU()\n","# Usage: applied to the output of a layer\n","output = relu(input)"],"metadata":{"id":"YuGwFMXUW-jS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Sigmoid Function\n","\n","The sigmoid function is commonly used in binary classification problems."],"metadata":{"id":"G4EGl3jMXF2G"}},{"cell_type":"code","source":["sigmoid = nn.Sigmoid()\n","# Usage: typically used in binary classification tasks\n","output = sigmoid(input)"],"metadata":{"id":"Zi_iPwGfXIpy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Tanh (Hyperbolic Tangent)\n","\n","Tanh (hyperbolic tangent) is another popular activation function often used in hidden layers."],"metadata":{"id":"Voz-8DqdXK45"}},{"cell_type":"code","source":["tanh = nn.Tanh()\n","# Usage: often used in hidden layers of a neural network\n","output = tanh(input)"],"metadata":{"id":"Wm_c7htiXM0P"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 3. Loss Functions\n","\n","Loss functions evaluate how well the model's predictions match the target values. They are crucial for training neural networks."],"metadata":{"id":"IfdMiLlJXOrq"}},{"cell_type":"markdown","source":["#### Cross-Entropy Loss\n","\n","Used primarily for classification tasks."],"metadata":{"id":"j3APEe1eXUcv"}},{"cell_type":"code","source":["cross_entropy_loss = nn.CrossEntropyLoss()\n","# Usage: commonly used in multi-class classification tasks\n","loss = cross_entropy_loss(predictions, targets)"],"metadata":{"id":"FP91AtMsXRAr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Mean Squared Error (MSE) Loss\n","\n","Often used in regression tasks to measure the average squared difference between predictions and targets."],"metadata":{"id":"fjemY95MXXme"}},{"cell_type":"code","source":["mse_loss = nn.MSELoss()\n","# Usage: frequently used in regression tasks\n","loss = mse_loss(predictions, targets)"],"metadata":{"id":"prC4rfNfXaJ8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 4. Utilities\n","\n","PyTorch provides utilities for tasks like weight initialization and model parameter management."],"metadata":{"id":"dvCOsmm2XcLf"}},{"cell_type":"markdown","source":["#### Weight Initialization\n","\n","In PyTorch, we can set the weights of the layer to be sampled from uniform or normal distribution using the uniform_ and normal_ functions within **``nn.init``** class."],"metadata":{"id":"2PJNL39hXlfq"}},{"cell_type":"code","source":["# Initializing weights of a linear layer with a custom function\n","def init_weights(m):\n","    if type(m) == nn.Linear:\n","        nn.init.normal_(m.weight, mean=0.0, std=0.1)\n","        nn.init.constant_(m.bias, 0)\n","\n","model = SimpleNet()\n","model.apply(init_weights)\n","[(name, param.shape) for name, param in model.named_parameters()]"],"metadata":{"id":"UcwVgOM5Xd6t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["[(name, param.shape) for name, param in model.named_parameters()]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ow78wPW0uepi","executionInfo":{"status":"ok","timestamp":1702681510982,"user_tz":300,"elapsed":503,"user":{"displayName":"Jiarui Tang","userId":"07730846287814974938"}},"outputId":"a8503373-2bda-4788-e1cd-9a54489ee834"},"execution_count":26,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('weight', torch.Size([1, 1])), ('bias', torch.Size([1]))]"]},"metadata":{},"execution_count":26}]},{"cell_type":"markdown","source":["#### Model Parameter Management\n","\n","Viewing and managing model parameters is essential for understanding and debugging the model."],"metadata":{"id":"jt5LUrKiXn5o"}},{"cell_type":"code","source":["# Viewing the parameters of a model\n","for param in model.parameters():\n","    print(param)"],"metadata":{"id":"HxCRVXeeXqTw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Defining the Network"],"metadata":{"id":"SCuYnXaT5eO6"}},{"cell_type":"code","source":["class SimpleNet(nn.Module):\n","    def __init__(self):\n","        super(SimpleNet, self).__init__()\n","        self.fc1 = nn.Linear(in_features=784, out_features=128)\n","        self.relu = nn.ReLU()\n","        self.fc2 = nn.Linear(128, 10)\n","\n","    def forward(self, x):\n","        x = self.fc1(x)\n","        x = self.relu(x)\n","        x = self.fc2(x)\n","        return x"],"metadata":{"id":"ltEYpZSa5fDd","executionInfo":{"status":"ok","timestamp":1702689669257,"user_tz":300,"elapsed":285,"user":{"displayName":"Jiarui Tang","userId":"07730846287814974938"}}},"execution_count":35,"outputs":[]},{"cell_type":"code","source":["# Create an instance of the network\n","model = SimpleNet()"],"metadata":{"id":"VLDi_fvGFDPz","executionInfo":{"status":"ok","timestamp":1702689671516,"user_tz":300,"elapsed":311,"user":{"displayName":"Jiarui Tang","userId":"07730846287814974938"}}},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":["Loss Function and Optimizer\n","\n","For our training example, let's use Cross-Entropy Loss and the Stochastic Gradient Descent (SGD) optimizer."],"metadata":{"id":"JvqfOjMGFMEt"}},{"cell_type":"code","source":["loss_function = nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"],"metadata":{"id":"oE92CbOPFMVJ","executionInfo":{"status":"ok","timestamp":1702689672761,"user_tz":300,"elapsed":4,"user":{"displayName":"Jiarui Tang","userId":"07730846287814974938"}}},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":["Generating Random Data\n","\n","Let's create some random input data and corresponding labels for demonstration purposes. Assuming we are working with flattened images of size 28x28 pixels (which is 784 elements):"],"metadata":{"id":"Dnr9Ah2gFT_a"}},{"cell_type":"code","source":["# Generating a batch of 5 random inputs and labels\n","batch_size = 5\n","input_data = torch.randn(batch_size, 784)  # Random input\n","labels = torch.randint(0, 10, (batch_size,))  # Random labels for 10 classes"],"metadata":{"id":"XOrjw_dwFUXK","executionInfo":{"status":"ok","timestamp":1702689674079,"user_tz":300,"elapsed":5,"user":{"displayName":"Jiarui Tang","userId":"07730846287814974938"}}},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":["Training Step\n","\n","Now, let's perform a single training step with this data."],"metadata":{"id":"yVrW-BpXFbHx"}},{"cell_type":"code","source":["# Zero the gradients\n","optimizer.zero_grad()\n","\n","# Forward pass: Compute predicted outputs by passing inputs to the model\n","predicted_outputs = model(input_data)\n","\n","# Compute the loss\n","loss = loss_function(predicted_outputs, labels)\n","\n","# Backward pass: compute gradient of the loss with respect to model parameters\n","loss.backward()\n","\n","# Perform a single optimization step (parameter update)\n","optimizer.step()\n","\n","print(f\"Loss: {loss.item()}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"taAul56pFZiL","executionInfo":{"status":"ok","timestamp":1702689679721,"user_tz":300,"elapsed":279,"user":{"displayName":"Jiarui Tang","userId":"07730846287814974938"}},"outputId":"0ccf1afd-74c8-454f-8f49-419087ed25f1"},"execution_count":39,"outputs":[{"output_type":"stream","name":"stdout","text":["Loss: 2.182338237762451\n"]}]},{"cell_type":"markdown","source":["Extend the training process to multiple epoches"],"metadata":{"id":"HYtNs2HRNIbH"}},{"cell_type":"code","source":["# Number of epochs\n","num_epochs = 30\n","\n","# Loop over the dataset multiple times\n","for epoch in range(num_epochs):\n","    running_loss = 0.0\n","\n","    # Zero the parameter gradients\n","    optimizer.zero_grad()\n","\n","    # Forward pass\n","    outputs = model(input_data)\n","\n","    # Calculate the loss\n","    loss = loss_function(outputs, labels)\n","\n","    # Backward pass and optimize\n","    loss.backward()\n","    optimizer.step()\n","\n","    # Print statistics\n","    running_loss += loss.item()\n","    print(f\"Epoch {epoch + 1}, Loss: {running_loss}\")\n","\n","print('Finished Training')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UNoAFpNvNOKa","executionInfo":{"status":"ok","timestamp":1702689688720,"user_tz":300,"elapsed":291,"user":{"displayName":"Jiarui Tang","userId":"07730846287814974938"}},"outputId":"5a798bdb-97ad-4fb7-8ba2-bf10e6ac4cf7"},"execution_count":40,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1, Loss: 1.9387896060943604\n","Epoch 2, Loss: 1.721871018409729\n","Epoch 3, Loss: 1.5288463830947876\n","Epoch 4, Loss: 1.3570845127105713\n","Epoch 5, Loss: 1.2006999254226685\n","Epoch 6, Loss: 1.058062195777893\n","Epoch 7, Loss: 0.9259008169174194\n","Epoch 8, Loss: 0.8084060549736023\n","Epoch 9, Loss: 0.7051030993461609\n","Epoch 10, Loss: 0.6156116724014282\n","Epoch 11, Loss: 0.5389310121536255\n","Epoch 12, Loss: 0.4732646346092224\n","Epoch 13, Loss: 0.41733837127685547\n","Epoch 14, Loss: 0.36961182951927185\n","Epoch 15, Loss: 0.32899096608161926\n","Epoch 16, Loss: 0.2943427562713623\n","Epoch 17, Loss: 0.2647152543067932\n","Epoch 18, Loss: 0.23925228416919708\n","Epoch 19, Loss: 0.21726855635643005\n","Epoch 20, Loss: 0.19820347428321838\n","Epoch 21, Loss: 0.18159647285938263\n","Epoch 22, Loss: 0.1670655608177185\n","Epoch 23, Loss: 0.15429317951202393\n","Epoch 24, Loss: 0.14301139116287231\n","Epoch 25, Loss: 0.1330145299434662\n","Epoch 26, Loss: 0.12410982698202133\n","Epoch 27, Loss: 0.1161460280418396\n","Epoch 28, Loss: 0.10900115966796875\n","Epoch 29, Loss: 0.10255952179431915\n","Epoch 30, Loss: 0.09673081338405609\n","Finished Training\n"]}]}]}