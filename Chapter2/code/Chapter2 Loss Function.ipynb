{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPiYdHq7H8V8DqUeCezXo8c"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Introduction to Loss Functions"],"metadata":{"id":"gzc39KW2phOi"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from math import log"],"metadata":{"id":"0oZOwLckp_zA","executionInfo":{"status":"ok","timestamp":1706043673899,"user_tz":300,"elapsed":4402,"user":{"displayName":"Jiarui Tang","userId":"07730846287814974938"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["## Loss Functions for Regression"],"metadata":{"id":"x7suBknDpnsr"}},{"cell_type":"markdown","source":["### Mean Squared Error (MSE) Loss in PyTorch\n","\n","Mean Squared Error (MSE) is a common loss function for regression problems. It calculates the average of the squares of the differences between the predicted and actual values. Below is an example of calculating MSE Loss using built-in function in PyTorch:"],"metadata":{"id":"In4xpLY0fCly"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"LMF47JqHpXng"},"outputs":[],"source":["predictions = torch.tensor([1.5, 2.5, 3.5])\n","targets = torch.tensor([1.0, 2.0, 3.0])\n","\n","# Mean Squared Error Loss\n","mse_loss = nn.MSELoss()\n","\n","# Calculate loss\n","loss = mse_loss(predictions, targets)\n","print(f\"MSE Loss: {loss.item()}\")"]},{"cell_type":"markdown","source":["### Mean Absolute Error (MAE) Loss in PyTorch\n","\n","Mean Absolute Error (MAE), also known as L1 loss, is another common loss function used in regression tasks. It calculates the average of the absolute differences between the predicted and actual values. Here's an example of PyTorch implementation:"],"metadata":{"id":"a1eN8b0vff4Z"}},{"cell_type":"code","source":["predictions = torch.tensor([1.5, 2.5, 3.5])\n","targets = torch.tensor([1.0, 2.0, 3.0])\n","\n","# Mean Absolute Error Loss\n","mae_loss = nn.L1Loss()\n","\n","# Calculate loss\n","loss = mae_loss(predictions, targets)\n","print(f\"MAE Loss: {loss.item()}\")"],"metadata":{"id":"WsvW1X0NfoWD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Loss Functions for Classification"],"metadata":{"id":"fwCW4PJ5pq6w"}},{"cell_type":"markdown","source":[" Below are the PyTorch code examples for Binary Cross-Entropy Loss, Cross-Entropy Loss, Kullback-Leibler Divergence Loss, and Negative Log Likelihood Loss:"],"metadata":{"id":"eca8zWSjpyhd"}},{"cell_type":"markdown","source":["### 1. Binary Cross-Entropy Loss\n","\n","For binary classification problems:"],"metadata":{"id":"iUhze71vp2lo"}},{"cell_type":"code","source":["# Example predictions and labels\n","predictions = torch.sigmoid(torch.randn(4))  # Sigmoid to get probabilities\n","labels = torch.tensor([1, 0, 1, 0], dtype=torch.float32)\n","\n","# Binary Cross-Entropy Loss\n","criterion = nn.BCELoss()\n","loss = criterion(predictions, labels)"],"metadata":{"id":"F-IewyGaptZo","executionInfo":{"status":"ok","timestamp":1705987423415,"user_tz":300,"elapsed":137,"user":{"displayName":"Jiarui Tang","userId":"07730846287814974938"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["predictions"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3dlqASMHvuBi","executionInfo":{"status":"ok","timestamp":1705987428854,"user_tz":300,"elapsed":164,"user":{"displayName":"Jiarui Tang","userId":"07730846287814974938"}},"outputId":"411c5993-1aa0-4fa4-e4fc-d9376f0d3e6d"},"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([0.6882, 0.4268, 0.6981, 0.6192])"]},"metadata":{},"execution_count":20}]},{"cell_type":"code","source":["labels"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZLrOzAUpvwCb","executionInfo":{"status":"ok","timestamp":1705987460992,"user_tz":300,"elapsed":172,"user":{"displayName":"Jiarui Tang","userId":"07730846287814974938"}},"outputId":"99dcc1e2-df0b-4911-db01-9d63a37d06b0"},"execution_count":24,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([1., 0., 1., 0.])"]},"metadata":{},"execution_count":24}]},{"cell_type":"code","source":["loss"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KEy4woBVvyMv","executionInfo":{"status":"ok","timestamp":1705987462369,"user_tz":300,"elapsed":232,"user":{"displayName":"Jiarui Tang","userId":"07730846287814974938"}},"outputId":"7305cc9d-657e-4fe6-e658-aba49d6779f9"},"execution_count":25,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(0.5638)"]},"metadata":{},"execution_count":25}]},{"cell_type":"code","source":["-1/len(labels)*sum(labels*torch.log(predictions)+(1-labels)*torch.log(1-predictions))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eTg-Dykfv2wV","executionInfo":{"status":"ok","timestamp":1705987456526,"user_tz":300,"elapsed":241,"user":{"displayName":"Jiarui Tang","userId":"07730846287814974938"}},"outputId":"78e3bf33-725c-41af-b85c-a235c5fc700d"},"execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(0.5638)"]},"metadata":{},"execution_count":23}]},{"cell_type":"markdown","source":["```python\n","criterion = nn.BCELoss()\n","loss = criterion(predictions, labels)\n","```\n","\n","- **Inputs**: The `predictions` should be probabilities (0 to 1), typically obtained with a sigmoid function.\n","- **Labels Format**: The `labels` should be binary (0 or 1) and match the shape of predictions.\n","- **Considerations**: Use `nn.BCEWithLogitsLoss` if the output layer of your model does not include a sigmoid.\n","\n","\n"],"metadata":{"id":"Qyn6KX49qLx-"}},{"cell_type":"markdown","source":["### 2. Cross-Entropy Loss\n","\n","For multi-class classification problems:"],"metadata":{"id":"KDRwyAMFtX6g"}},{"cell_type":"code","source":["# Example predictions and labels\n","predictions = torch.randn(4, 5)  # 4 samples, 5 class predictions\n","labels = torch.tensor([1, 0, 3, 2], dtype=torch.long)  # Class labels for each sample\n","\n","# Cross-Entropy Loss\n","criterion = nn.CrossEntropyLoss()\n","loss = criterion(predictions, labels)"],"metadata":{"id":"TcFoR5WTs-hC","executionInfo":{"status":"ok","timestamp":1705990840109,"user_tz":300,"elapsed":184,"user":{"displayName":"Jiarui Tang","userId":"07730846287814974938"}}},"execution_count":60,"outputs":[]},{"cell_type":"code","source":["predictions"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6UlSdvdo3PgK","executionInfo":{"status":"ok","timestamp":1705990842862,"user_tz":300,"elapsed":244,"user":{"displayName":"Jiarui Tang","userId":"07730846287814974938"}},"outputId":"18cfcb04-c77f-4c16-f653-11666068f8bf"},"execution_count":61,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 0.7114, -0.9152, -0.3899,  0.9435, -0.1819],\n","        [-0.6162, -0.0346, -0.5723, -1.1487, -1.4400],\n","        [ 0.3388, -0.9662, -0.2876, -0.1088, -0.1857],\n","        [-1.3066,  1.5789, -0.3382, -0.6364,  2.9867]])"]},"metadata":{},"execution_count":61}]},{"cell_type":"code","source":["labels"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GiLvhssn3RNS","executionInfo":{"status":"ok","timestamp":1705990844860,"user_tz":300,"elapsed":171,"user":{"displayName":"Jiarui Tang","userId":"07730846287814974938"}},"outputId":"62c9ac64-9c62-46a7-c115-cba577ac59e3"},"execution_count":62,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([1, 0, 3, 2])"]},"metadata":{},"execution_count":62}]},{"cell_type":"code","source":["loss"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WLQ_IQcp3SVW","executionInfo":{"status":"ok","timestamp":1705990847160,"user_tz":300,"elapsed":187,"user":{"displayName":"Jiarui Tang","userId":"07730846287814974938"}},"outputId":"45b06b8b-c96f-4af0-9edc-41b4f0f7e0ee"},"execution_count":63,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(2.3831)"]},"metadata":{},"execution_count":63}]},{"cell_type":"code","source":["softmax_predictions = torch.softmax(predictions, dim=1)\n","softmax_predictions"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3piYOwGt4qMd","executionInfo":{"status":"ok","timestamp":1705991173941,"user_tz":300,"elapsed":184,"user":{"displayName":"Jiarui Tang","userId":"07730846287814974938"}},"outputId":"13f2959d-f2c3-439f-d5d6-6ba9a3734950"},"execution_count":66,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0.3125, 0.0614, 0.1039, 0.3942, 0.1279],\n","        [0.2058, 0.3681, 0.2150, 0.1208, 0.0903],\n","        [0.3293, 0.0893, 0.1760, 0.2105, 0.1949],\n","        [0.0103, 0.1852, 0.0272, 0.0202, 0.7570]])"]},"metadata":{},"execution_count":66}]},{"cell_type":"code","source":["-1/4*sum(torch.tensor([torch.log(softmax_predictions[i][labels[i]]) for i in range(len(labels))]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kFK21hMH3TYR","executionInfo":{"status":"ok","timestamp":1705991241096,"user_tz":300,"elapsed":155,"user":{"displayName":"Jiarui Tang","userId":"07730846287814974938"}},"outputId":"c715edbf-e8ef-4c23-be35-647a93245394"},"execution_count":69,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(2.3831)"]},"metadata":{},"execution_count":69}]},{"cell_type":"markdown","source":["```python\n","criterion = nn.CrossEntropyLoss()\n","loss = criterion(predictions, labels)\n","```\n","\n","- **Inputs**: The `predictions` (also known as logits) should not be passed through a softmax layer before this loss function, as `nn.CrossEntropyLoss` applies softmax internally.\n","- **Labels Format**: The `labels` should contain the class indices (not one-hot encoded) and should be of type `torch.long`.\n","- **Considerations**: This loss function is more numerically stable than using a separate softmax followed by a negative log likelihood loss."],"metadata":{"id":"0cbFulrnt2sd"}},{"cell_type":"markdown","source":["### 3. Kullback-Leibler Divergence Loss\n","\n","For comparing two probability distributions:"],"metadata":{"id":"D5Nl5BTmtmA1"}},{"cell_type":"code","source":["# Example predicted and target distributions\n","predicted_log_probs = torch.log_softmax(torch.randn(4, 5), dim=1)  # Log probabilities\n","true_probs = torch.softmax(torch.randn(4, 5), dim=1)\n","\n","# Kullback-Leibler Divergence Loss\n","criterion = nn.KLDivLoss(reduction='batchmean')\n","loss = criterion(predicted_log_probs, true_probs)"],"metadata":{"id":"lGdK81eYtpIh","executionInfo":{"status":"ok","timestamp":1705994067047,"user_tz":300,"elapsed":179,"user":{"displayName":"Jiarui Tang","userId":"07730846287814974938"}}},"execution_count":77,"outputs":[]},{"cell_type":"code","source":["predicted_log_probs"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kNuxSFW6K2g2","executionInfo":{"status":"ok","timestamp":1705994067845,"user_tz":300,"elapsed":8,"user":{"displayName":"Jiarui Tang","userId":"07730846287814974938"}},"outputId":"b3e0d488-76e0-4ba3-8e1a-6db79b965203"},"execution_count":78,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[-4.4963, -1.2036, -1.5853, -1.4722, -1.3688],\n","        [-2.6413, -2.2889, -2.2535, -0.5027, -2.1419],\n","        [-2.3875, -1.0028, -2.9582, -3.1580, -0.8055],\n","        [-2.3296, -0.4341, -2.2907, -2.1956, -3.1619]])"]},"metadata":{},"execution_count":78}]},{"cell_type":"code","source":["true_probs"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oTjfNlAjK3_j","executionInfo":{"status":"ok","timestamp":1705994070144,"user_tz":300,"elapsed":158,"user":{"displayName":"Jiarui Tang","userId":"07730846287814974938"}},"outputId":"30bb612a-20c1-431d-fea3-1c64d390c673"},"execution_count":79,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0.0345, 0.1775, 0.1944, 0.2313, 0.3622],\n","        [0.1333, 0.6237, 0.1228, 0.0795, 0.0406],\n","        [0.3026, 0.1969, 0.0661, 0.2365, 0.1979],\n","        [0.1164, 0.4287, 0.2206, 0.1695, 0.0648]])"]},"metadata":{},"execution_count":79}]},{"cell_type":"code","source":["loss"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SWLvCzE1JJxZ","executionInfo":{"status":"ok","timestamp":1705994071840,"user_tz":300,"elapsed":142,"user":{"displayName":"Jiarui Tang","userId":"07730846287814974938"}},"outputId":"963a795f-e8d3-4218-9356-83b3dfc1cc6d"},"execution_count":80,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(0.4276)"]},"metadata":{},"execution_count":80}]},{"cell_type":"code","source":["1/predicted_log_probs.shape[0]*sum(sum(true_probs*(torch.log(true_probs)-predicted_log_probs)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z9VTvHT-M3Mi","executionInfo":{"status":"ok","timestamp":1705995025625,"user_tz":300,"elapsed":156,"user":{"displayName":"Jiarui Tang","userId":"07730846287814974938"}},"outputId":"b758d5b9-28ac-4305-ee98-e88699451cf3"},"execution_count":93,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(0.4276)"]},"metadata":{},"execution_count":93}]},{"cell_type":"markdown","source":["```python\n","criterion = nn.KLDivLoss(reduction='batchmean')\n","loss = criterion(predicted_log_probs, true_probs)\n","```\n","\n","- **Inputs**: `predicted_log_probs` should be log probabilities (use `torch.log_softmax`). The `true_probs` should be probabilities (use `torch.softmax` or equivalent).\n","- **Output Range**: The KL divergence can output a wider range of values compared to other loss functions, which might affect learning rates and convergence.\n","- **Considerations**: Ensure the dimensionality over which softmax and log-softmax are applied matches the expected dimensionality of the loss function."],"metadata":{"id":"nnZi-K5zuGiQ"}},{"cell_type":"markdown","source":["### 4. Negative Log Likelihood Loss\n","\n","Often used in combination with a log-softmax layer:"],"metadata":{"id":"wl6CGbxfttKI"}},{"cell_type":"code","source":["# Example log probabilities and labels\n","log_probs = torch.log_softmax(torch.randn(4, 5), dim=1)  # Log probabilities from log-softmax\n","labels = torch.tensor([1, 0, 3, 2])  # Class labels for each sample\n","\n","# Negative Log Likelihood Loss\n","criterion = nn.NLLLoss()\n","loss = criterion(log_probs, labels)"],"metadata":{"id":"ii3drtvJtvfk","executionInfo":{"status":"ok","timestamp":1705996105383,"user_tz":300,"elapsed":183,"user":{"displayName":"Jiarui Tang","userId":"07730846287814974938"}}},"execution_count":94,"outputs":[]},{"cell_type":"code","source":["log_probs"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dgstvMiZSnXA","executionInfo":{"status":"ok","timestamp":1705996106324,"user_tz":300,"elapsed":150,"user":{"displayName":"Jiarui Tang","userId":"07730846287814974938"}},"outputId":"12b6b2ba-bb41-43fd-c2d7-fe7a940ea138"},"execution_count":95,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[-2.1952, -2.0639, -0.3655, -4.2204, -2.9338],\n","        [-2.2309, -1.6147, -1.2787, -1.6363, -1.5118],\n","        [-2.1938, -3.0970, -1.5740, -1.3624, -0.9675],\n","        [-1.9533, -1.1320, -3.2088, -1.2513, -1.5640]])"]},"metadata":{},"execution_count":95}]},{"cell_type":"code","source":["labels"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XeQe8FsKSooE","executionInfo":{"status":"ok","timestamp":1705996108524,"user_tz":300,"elapsed":157,"user":{"displayName":"Jiarui Tang","userId":"07730846287814974938"}},"outputId":"fb613764-22a9-4d2c-dea1-ecd3dc43a6d6"},"execution_count":96,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([1, 0, 3, 2])"]},"metadata":{},"execution_count":96}]},{"cell_type":"code","source":["loss"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_qd7b6VGSp4e","executionInfo":{"status":"ok","timestamp":1705996109872,"user_tz":300,"elapsed":189,"user":{"displayName":"Jiarui Tang","userId":"07730846287814974938"}},"outputId":"8ef5ffd1-36d9-4f6f-f295-07e46c28564f"},"execution_count":97,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(2.2165)"]},"metadata":{},"execution_count":97}]},{"cell_type":"code","source":["-1/4*(-2.0639-2.2309-1.3624-3.2088)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ILH5QEntUA0T","executionInfo":{"status":"ok","timestamp":1705996506447,"user_tz":300,"elapsed":126,"user":{"displayName":"Jiarui Tang","userId":"07730846287814974938"}},"outputId":"cbc656e2-5a36-4da0-a547-7dba2cfda4bc"},"execution_count":103,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2.2165"]},"metadata":{},"execution_count":103}]},{"cell_type":"code","source":["criterion(torch.log_softmax(predictions, dim=1), labels)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WCJ5OKgjTmks","executionInfo":{"status":"ok","timestamp":1705996427455,"user_tz":300,"elapsed":161,"user":{"displayName":"Jiarui Tang","userId":"07730846287814974938"}},"outputId":"002f5c93-24ea-4564-cd64-2b1515a17fde"},"execution_count":102,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(2.3831)"]},"metadata":{},"execution_count":102}]},{"cell_type":"markdown","source":["Compare the output with the cross entropy loss results, what did you find?"],"metadata":{"id":"t_vH32qRTp74"}},{"cell_type":"markdown","source":["```python\n","criterion = nn.NLLLoss()\n","loss = criterion(log_probs, labels)\n","```\n","\n","- **Inputs**: The input should be log probabilities, typically obtained by applying `torch.log_softmax` to the neural network's output.\n","- **Labels Format**: Similar to `nn.CrossEntropyLoss`, the `labels` should contain class indices and be of type `torch.long`.\n","- **Usage Scenario**: Often used in conjunction with a network ending in a log_softmax layer. It's essentially like `CrossEntropyLoss` but requires the preceding layer to output log probabilities."],"metadata":{"id":"aTcD0YhruZjj"}},{"cell_type":"markdown","source":["When implementing these loss functions, it's crucial to ensure that the input data and network architecture are compatible with the specific requirements of each function. Additionally, understanding the underlying mathematical principles can aid in debugging and fine-tuning the model training process."],"metadata":{"id":"CAf0rPCGuyI4"}},{"cell_type":"markdown","source":["## Create Custom Loss Function in PyTorch"],"metadata":{"id":"qeQXBuZDXGbq"}},{"cell_type":"markdown","source":["### Creating custom loss function as a python function\n","\n","Below, we'll create a simple Cross-Entropy Loss function:"],"metadata":{"id":"w0DCQWVJXPaY"}},{"cell_type":"code","source":["def custom_cross_entropy_loss(y_pred, y_true):\n","  \"\"\"\n","  Custom Cross-Entropy Loss implementation using PyTorch.\n","  :param y_pred: PyTorch tensor of predicted logits (not probabilities).\n","  :param y_true: PyTorch tensor of true labels.\n","  :return: Cross-entropy loss.\n","  \"\"\"\n","  #Specifying the batch size\n","  my_batch_size = y_pred.size()[0]\n","\n","  #Get the log probabilities values\n","  log_probabilities = torch.log_softmax(y_pred, dim=1)\n","\n","  #Pick the probabilities corresponding to the true labels\n","  relevant_log_probs = log_probabilities[range(my_batch_size), y_true]\n","\n","  #Take the negative and mean of these log probabilities\n","  loss = -torch.mean(relevant_log_probs)\n","  return loss"],"metadata":{"id":"lwL3hOL0usrd","executionInfo":{"status":"ok","timestamp":1706028534410,"user_tz":300,"elapsed":5,"user":{"displayName":"Jiarui Tang","userId":"07730846287814974938"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# Example usage\n","y_pred = torch.tensor([[1.5, 0.5, -0.5], [-0.5, 1.5, 0.5], [0.5, -0.5, 1.5]])  # Predicted logits for 3 classes\n","y_true = torch.tensor([0, 1, 2])  # True labels\n","\n","loss = custom_cross_entropy_loss(y_pred, y_true)\n","print(\"Custom Cross-Entropy Loss:\", loss.item())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JZZ7OWKkZNO9","executionInfo":{"status":"ok","timestamp":1706028536253,"user_tz":300,"elapsed":179,"user":{"displayName":"Jiarui Tang","userId":"07730846287814974938"}},"outputId":"09dd6b18-ea5b-4394-8426-cae1d20ee333"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Custom Cross-Entropy Loss: 0.40760597586631775\n"]}]},{"cell_type":"code","source":["criterion = nn.CrossEntropyLoss()\n","loss_fn = criterion(y_pred, y_true)\n","print(\"Cross-Entropy Loss using nn.CrossEntropyLoss():\",loss_fn.item())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xc3SkxdFZz8b","executionInfo":{"status":"ok","timestamp":1706028537269,"user_tz":300,"elapsed":5,"user":{"displayName":"Jiarui Tang","userId":"07730846287814974938"}},"outputId":"ae1f409f-8273-4f0b-dde3-8206cfe11293"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Cross-Entropy Loss using nn.CrossEntropyLoss(): 0.40760597586631775\n"]}]},{"cell_type":"markdown","source":["### Creating custom loss function with a class definition\n","\n","Let's create a custom dice loss function, which computes the similarity between two samples, to act as a loss function for binary classification problems:"],"metadata":{"id":"8zA44hPLakg-"}},{"cell_type":"code","source":["class CustomCrossEntropyLoss(nn.Module):\n","    def __init__(self):\n","        \"\"\"\n","        Constructor for the custom Cross-Entropy Loss.\n","        \"\"\"\n","        super(CustomCrossEntropyLoss, self).__init__()\n","\n","    def forward(self, y_pred, y_true):\n","        \"\"\"\n","        Forward pass for the custom Cross-Entropy Loss.\n","\n","        :param y_pred: Tensor of predicted logits (not probabilities).\n","        :param y_true: Tensor of ground truth labels.\n","        :return: Computed Cross-Entropy Loss.\n","        \"\"\"\n","        # Ensuring the predicted values are in log form probabilities\n","        log_probs = torch.log_softmax(y_pred, dim=1)\n","\n","        # Picking the log probabilities corresponding to true labels\n","        relevant_log_probs = log_probs[range(len(y_true)), y_true]\n","\n","        # Negative log likelihood loss\n","        loss = -torch.mean(relevant_log_probs)\n","        return loss"],"metadata":{"id":"oPD0OZSvHL10","executionInfo":{"status":"ok","timestamp":1706028540055,"user_tz":300,"elapsed":4,"user":{"displayName":"Jiarui Tang","userId":"07730846287814974938"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["Let's compute the result of this custom cross entropy loss function with class definition using the same generated data in \"Creating custom loss function as a python function\" section. Compare the result with the loss computed by `custom_cross_entropy_loss` and `nn.CrossEntropyLoss`."],"metadata":{"id":"b_Xo-ViKOhcr"}},{"cell_type":"code","source":["loss_function = CustomCrossEntropyLoss()\n","loss = loss_function(y_pred, y_true)\n","print(\"Custom Cross-Entropy Loss:\", loss.item())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QJbiL4NsOOBa","executionInfo":{"status":"ok","timestamp":1706028541700,"user_tz":300,"elapsed":148,"user":{"displayName":"Jiarui Tang","userId":"07730846287814974938"}},"outputId":"2855b082-be47-4955-8ba7-7d050df081cb"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Custom Cross-Entropy Loss: 0.40760597586631775\n"]}]},{"cell_type":"markdown","source":["## Dice Loss function for Image Segmentation"],"metadata":{"id":"DonS_m0BHN28"}},{"cell_type":"markdown","source":["Image segmentation is a critical task in the field of computer vision, where the objective is to classify each pixel of an image into a particular class. A common challenge in image segmentation is dealing with class imbalance, which can adversely affect the performance of segmentation models. As the commonly used loss function for multi-class classification, cross entropy loss is sensitive to imbalanced data. Dice Loss became a popular loss function used to tackle this issue, especially in medical image segmentation.\n","\n","Dice Loss is based on the Dice Coefficient, a statistical tool used to gauge the similarity of two samples. This coefficient is particularly useful in evaluating the similarity between the predicted segmentation and the ground truth.\n","\n","### Dice Coefficient\n","\n","The Dice Coefficient, also known as the Sørensen-Dice index or Dice's Coefficient, is defined as:\n","\n","$$\\small{\\text{Dice Coefficient}=\\frac{2×|X\\cap Y|}{|X|+|Y|}}$$\n","\n","where $\\small X$ and $\\small Y$ are two sets of samples. In the context of image segmentation, $\\small X$ can be the predicted segmentation and $\\small Y$ the ground truth.\n","\n","In the case of estimating a Dice coefficient on predicted segmentation masks, we can approximate $\\small|X\\cap Y|$ as the element-wise multiplication between the prediction and target mask, and then sum the resulting matrix. The prediction mask is usually the predicted probability output by `sigmoid` or `softmax` function, ranging from 0 to 1.\n","\n","A small constant (smooth) is added to the numerator and denominator to ensure numerical stability, particularly to avoid division by zero.\n"],"metadata":{"id":"fQVNZfgTIYPS"}},{"cell_type":"markdown","source":["### Dice Loss\n","\n","Dice Loss is formulated as:\n","\n","$$\\small\\text{Dice Loss}=\n","\\text{Dice Loss}=1−\\text{Dice Coefficient}$$\n","\n","**This loss function is particularly useful for datasets with class imbalance**, as it ensures that the model does not bias towards the majority class.\n"],"metadata":{"id":"dKzq1bb7MekR"}},{"cell_type":"markdown","source":["### Dice Loss Function Implementation in PyTorch\n","\n","Let's create a custom dice loss function, which computes the similarity between two samples, to act as a loss function for binary classification problems:"],"metadata":{"id":"rzBq2z8UMgEt"}},{"cell_type":"code","source":["class DiceLoss(nn.Module):\n","    def __init__(self):\n","        \"\"\"\n","        Constructor for Dice Loss.\n","        \"\"\"\n","        super(DiceLoss, self).__init__()\n","\n","    def forward(self, y_pred, y_true, smooth=1.0):\n","        \"\"\"\n","        Forward pass for Dice Loss.\n","\n","        :param y_pred: Tensor of predicted outputs (after activation).\n","        :param y_true: Tensor of ground truth labels.\n","        :param smooth: A smoothing constant to avoid division by zero.\n","        :return: Computed Dice Loss.\n","        \"\"\"\n","        # Check the sizes of y_pred and y_true are consistent\n","        assert y_pred.size() == y_true.size(), \"Predicted and ground truth labels have different shapes\"\n","\n","        # Flatten label and prediction tensors\n","        y_pred = y_pred.view(-1)\n","        y_true = y_true.view(-1)\n","\n","        intersection = (y_pred * y_true).sum()\n","        dice_coeff = (2. * intersection + smooth) / (y_pred.square().sum() + y_true.square().sum() +smooth)\n","\n","        return 1.0 - dice_coeff"],"metadata":{"id":"USuANLO6d44B","executionInfo":{"status":"ok","timestamp":1706043674115,"user_tz":300,"elapsed":5,"user":{"displayName":"Jiarui Tang","userId":"07730846287814974938"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# Example usage\n","y_pred = torch.sigmoid(torch.randn(1, 1, 5, 5))  # Example predicted mask\n","y_true = torch.tensor([[[[1, 0, 0, 0, 1], [0, 1, 0, 0, 1], [0, 0, 1, 0, 1], [0, 0, 0, 1, 1], [1, 1, 1, 1, 1]]]])  # Example true mask\n","\n","dice_loss = DiceLoss()\n","loss = dice_loss(y_pred, y_true)\n","print(\"Dice Loss:\", loss.item())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mvZGvmqneHZs","executionInfo":{"status":"ok","timestamp":1706043679514,"user_tz":300,"elapsed":156,"user":{"displayName":"Jiarui Tang","userId":"07730846287814974938"}},"outputId":"b53b2b94-6ec5-4443-8580-635360bf4a80"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Dice Loss: 0.3842737078666687\n"]}]},{"cell_type":"code","source":["y_pred"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"femdCpXA-7Hv","executionInfo":{"status":"ok","timestamp":1706043680559,"user_tz":300,"elapsed":168,"user":{"displayName":"Jiarui Tang","userId":"07730846287814974938"}},"outputId":"757bcb60-0631-4bf8-efa9-a8cfc3685982"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[[0.2829, 0.6562, 0.3265, 0.1825, 0.5295],\n","          [0.7689, 0.6150, 0.3891, 0.7714, 0.6671],\n","          [0.5337, 0.5505, 0.9265, 0.2700, 0.8181],\n","          [0.4257, 0.7626, 0.5778, 0.6394, 0.4290],\n","          [0.8714, 0.5705, 0.6270, 0.5504, 0.7423]]]])"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["y_true"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Va_qc8Cq--09","executionInfo":{"status":"ok","timestamp":1706043681632,"user_tz":300,"elapsed":160,"user":{"displayName":"Jiarui Tang","userId":"07730846287814974938"}},"outputId":"7c57b951-cca7-48b1-b197-f51e3615cdd6"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[[1, 0, 0, 0, 1],\n","          [0, 1, 0, 0, 1],\n","          [0, 0, 1, 0, 1],\n","          [0, 0, 0, 1, 1],\n","          [1, 1, 1, 1, 1]]]])"]},"metadata":{},"execution_count":7}]},{"cell_type":"markdown","source":["### Dice Loss Function Usage Example - Image Segmentation\n","\n","In this section, under a simple setup, we use synthetic data to implement an example of image segmentation tasks. We'll create random tensors to simulate images and segmentation masks, and then apply the Dice Loss function.\n","\n","Through the script below, you'll see how the self-defined Dice Loss function is used in practice with PyTorch for a segmentation task. Keep in mind that this is a simplified example primarily for illustrative purposes."],"metadata":{"id":"Kk8VLYkrc5No"}},{"cell_type":"code","source":["# Import necessary libraries\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, TensorDataset\n","\n","# Dice Loss Class Definition\n","class DiceLoss(nn.Module):\n","    def __init__(self):\n","        super(DiceLoss, self).__init__()\n","\n","    def forward(self, inputs, targets, smooth=1):\n","        inputs = inputs.view(-1)\n","        targets = targets.view(-1)\n","\n","        intersection = (inputs * targets).sum()\n","        dice = (2.*intersection + smooth)/(inputs.square().sum() + targets.square().sum() + smooth)\n","\n","        return 1 - dice\n","\n","# Dummy Convolutional Neural Network for Segmentation\n","class DummySegmentationModel(nn.Module):\n","    def __init__(self):\n","        super(DummySegmentationModel, self).__init__()\n","        self.conv1 = nn.Conv2d(1, 1, kernel_size=3, padding=1)\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        x = self.conv1(x)\n","        x = self.sigmoid(x)\n","        return x\n","\n","# Synthetic Dataset Generation\n","def generate_synthetic_data(batch_size=10, height=256, width=256):\n","    # Generating random images and masks\n","    images = torch.rand(batch_size, 1, height, width)  # 1 channel images\n","    masks = torch.randint(0, 2, (batch_size, 1, height, width)).float()  # Binary masks\n","    return images, masks\n","\n","# Prepare Data Loader\n","batch_size = 5\n","images, masks = generate_synthetic_data(batch_size)\n","dataset = TensorDataset(images, masks)\n","data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n","\n","# Model, Loss Function, and Optimizer\n","model = DummySegmentationModel()\n","dice_loss = DiceLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","# Training Loop\n","num_epochs = 2\n","for epoch in range(num_epochs):\n","    for batch_images, batch_masks in data_loader:\n","        optimizer.zero_grad()\n","\n","        # Forward pass\n","        outputs = model(batch_images)\n","\n","        # Loss computation\n","        loss = dice_loss(outputs, batch_masks)\n","\n","        # Backward pass and optimization\n","        loss.backward()\n","        optimizer.step()\n","\n","    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n","\n","print(\"Done.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ObXEkAhadCR6","executionInfo":{"status":"ok","timestamp":1706049444399,"user_tz":300,"elapsed":120,"user":{"displayName":"Jiarui Tang","userId":"07730846287814974938"}},"outputId":"78551ce3-55f5-44dc-b99a-f959d2b51d4a"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [1/2], Loss: 0.3873\n","Epoch [2/2], Loss: 0.3864\n","Done.\n"]}]}]}