{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2bcbf6-6ee1-40bc-adf9-d22dfe3a467f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7dfbc0-70d8-46a9-95f5-3aee30e0f39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data\n",
    "subject_ids = np.loadtxt(\"selected_samples_subset.txt\", dtype=str)\n",
    "left_hippo_dir = \"LeftCSV_subset/\"\n",
    "left_hippo_files = glob.glob(left_hippo_dir+\"*\")\n",
    "right_hippo_dir = \"RightCSV_subset/\"\n",
    "right_hippo_files = glob.glob(right_hippo_dir+\"*\")\n",
    "labels = pd.read_csv(\"adni_subset.csv\",names=['ID','AD']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94354126-ea65-4bf2-af98-925250c46240",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32194dd-e2be-4c21-8654-4755095aaade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dataset\n",
    "class HippocampusDataset(Dataset):\n",
    "    def __init__(self, left_hippo_files, right_hippo_files, labels=None):\n",
    "        self.left_hippo = [pd.read_csv(f, header=None, sep = \" \").values for f in left_hippo_files]\n",
    "        self.right_hippo = [pd.read_csv(f, header=None, sep = \" \").values for f in right_hippo_files]\n",
    "        self.labels = labels['AD']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.left_hippo)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        left = self.left_hippo[idx]\n",
    "        right = self.right_hippo[idx]\n",
    "        left = left.astype(np.float32)\n",
    "        right = right.astype(np.float32)\n",
    "        ##########################################################\n",
    "        # TODO: Perform the following data preprocessing steps:\n",
    "        # 1. Concatenate the left and right hippocampus data along the feature dimension to create a sample of shape (15000, 14).\n",
    "        # 2. Transpose the sample to have a shape of (14, 15000).\n",
    "        # 3. Convert the sample into a PyTorch tensor with dtype torch.float32.\n",
    "        # 4. Normalize each feature channel by subtracting its mean and dividing by its standard deviation.\n",
    "        ##############################################################\n",
    "        # Replace \"pass\" statement with your code\n",
    "\n",
    "        # Step 1: Concatenate left and right data\n",
    "        sample = #pass\n",
    "\n",
    "        # Step 2: Transpose the sample\n",
    "        sample = #pass\n",
    "\n",
    "        # Step 3: Convert to PyTorch tensor\n",
    "        sample = #pass\n",
    "\n",
    "        # Step 4: Normalize each feature channel\n",
    "        mean = #pass\n",
    "        std = #pass\n",
    "        sample = #pass\n",
    "\n",
    "        #############################################################\n",
    "        # END OF YOUR CODE\n",
    "        ##########################################################\n",
    "\n",
    "\n",
    "        if self.labels is not None:\n",
    "            label = self.labels.iloc[idx]\n",
    "            return sample, label\n",
    "        else:\n",
    "            return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee1b89d-e771-4f08-86dc-ba5936cd3aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated Model Class using 1D Convolutions\n",
    "class HippoCNN(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super(HippoCNN, self).__init__()\n",
    "        ##########################################################\n",
    "        # TODO: Initialize the following layers:\n",
    "        # 1. Three 1D convolutional layers\n",
    "        # 2. A MaxPool1d layer self.pool \n",
    "        # 3. A Dropout layer self.dropout with dropout probability 0.5\n",
    "        # 4. Two fully connected layers:\n",
    "        # Note: Ensure that you compute the correct input size for self.fc1 based on the output length after convolution and pooling layers.\n",
    "        ##############################################################\n",
    "        # Replace \"pass\" statement with your code\n",
    "\n",
    "        self.conv1 = #pass\n",
    "        self.conv2 = #pass\n",
    "        self.conv3 = #pass\n",
    "        self.pool = #pass\n",
    "        self.dropout = #pass\n",
    "        self.fc1 = #pass\n",
    "        self.fc2 = #pass\n",
    "\n",
    "        #############################################################\n",
    "        # END OF YOUR CODE\n",
    "        ##########################################################\n",
    "\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "    ##########################################################\n",
    "    # TODO: Implement helper function `calculate_output_length` to compute the output length after a series of convolutional and pooling layers.\n",
    "    # Steps:\n",
    "    # 1. Initialize a variable `length` with the initial input length (e.g., 15000).\n",
    "    # 2. Define a list `layers` containing tuples for each layer with the format:\n",
    "    #    ('layer_type', kernel_size, stride)\n",
    "    #    where `layer_type` is either 'conv' or 'pool'.\n",
    "    # 3. For each layer in `layers`, update `length` using the output size formula:\n",
    "    #    length = floor(((length - (kernel_size - 1) - 1) / stride) + 1)\n",
    "    #    Note: Assume padding=0 and dilation=1.\n",
    "    # 4. Return the final computed `length`.\n",
    "    ##############################################################\n",
    "    # Replace \"pass\" statement with your code\n",
    "\n",
    "    def calculate_output_length(self):\n",
    "        #pass\n",
    "        return length\n",
    "\n",
    "    #############################################################\n",
    "    # END OF YOUR CODE\n",
    "    ##########################################################\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))  \n",
    "        x = self.pool(x)           \n",
    "        x = F.relu(self.conv2(x))  \n",
    "        x = self.pool(x)          \n",
    "        x = F.relu(self.conv3(x)) \n",
    "        x = self.pool(x)           \n",
    "\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x.squeeze()\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        data, labels = batch  \n",
    "        outputs = self(data) \n",
    "        loss = self.criterion(outputs, labels)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        acc = (preds == labels).float().mean()\n",
    "        self.log('train_loss', loss)\n",
    "        self.log('train_acc', acc)\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        data, labels = batch\n",
    "        outputs = self(data)\n",
    "        loss = self.criterion(outputs, labels)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        acc = (preds == labels).float().mean()\n",
    "        self.log('val_loss', loss)\n",
    "        self.log('val_acc', acc)\n",
    "        \n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d285ea4e-0725-4cbd-9082-33c76879c73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.DataFrame({\n",
    "    'ID': subject_ids,\n",
    "    'LeftFile': left_hippo_files,\n",
    "    'RightFile': right_hippo_files\n",
    "})\n",
    "\n",
    "# Merge data and labels on SubjectID\n",
    "merged_df = pd.merge(data_df, labels, on='ID', how='inner')  # Keep only subjects with labels\n",
    "\n",
    "# Now extract the filtered lists\n",
    "filtered_left_files = merged_df['LeftFile'].tolist()\n",
    "filtered_right_files = merged_df['RightFile'].tolist()\n",
    "\n",
    "unique_labels = sorted(set(labels['AD']))\n",
    "label_to_index = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "labels['AD'] = [label_to_index[label] for label in labels['AD']]\n",
    "\n",
    "# Split into training and validation sets\n",
    "train_left, test_left, train_right, test_right, train_labels, test_labels = train_test_split(\n",
    "    filtered_left_files, filtered_right_files, labels, test_size=0.2, random_state=42\n",
    ")\n",
    "train_left, val_left, train_right, val_right, train_labels, val_labels = train_test_split(\n",
    "    train_left, train_right, train_labels, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4622ab25-5f3b-48d7-a7a2-1ff81eff4c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Datasets and DataLoaders\n",
    "train_dataset = HippocampusDataset(train_left, train_right, train_labels)\n",
    "val_dataset = HippocampusDataset(val_left, val_right, val_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8)\n",
    "\n",
    "# Initialize model\n",
    "model = HippoCNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7becd605-353c-44ef-a15e-32888d078853",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = HippocampusDataset(val_left, val_right, val_labels, transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8560843-a4f0-4b11-a1a1-b1478c6b2a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_left[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cf4ba9-0e39-41ba-8134-e900756e8ab0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Trainer\n",
    "trainer = pl.Trainer(max_epochs=50)\n",
    "\n",
    "# Training\n",
    "trainer.fit(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abdb1da-ccbe-4462-a6e2-32fda5c87e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = HippocampusDataset(test_left, test_right, test_labels)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c96741-d4e2-4b42-b50f-020973924e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference on Test Set\n",
    "model.eval()\n",
    "test_preds = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        data, _ = batch  # Unpack the batch; labels are ignored\n",
    "        outputs = model(data)\n",
    "        predictions = torch.argmax(outputs, dim=1)\n",
    "        test_preds.extend(predictions.cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8f2293-1a23-4300-9891-5ccffd51ae72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Metrics\n",
    "\n",
    "labels_list = test_labels['AD']\n",
    "\n",
    "##########################################################\n",
    "# TODO: Import the necessary evaluation metrics from sklearn.metrics:\n",
    "# - accuracy_score\n",
    "# - precision_score\n",
    "# - recall_score\n",
    "# - f1_score\n",
    "# - roc_auc_score\n",
    "#\n",
    "# Then, compute the following metrics using `labels_list` and `test_preds`:\n",
    "# 1. Accuracy\n",
    "# 2. Precision (use 'weighted' average)\n",
    "# 3. Recall (use 'weighted' average)\n",
    "# 4. F1-Score (use 'weighted' average)\n",
    "# 5. ROC-AUC (use 'weighted' average and 'ovr' for multi_class)\n",
    "#\n",
    "# Finally, print out each metric with four decimal places in the specified format.\n",
    "##############################################################\n",
    "# Replace \"pass\" statement with your code\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "accuracy = #pass\n",
    "precision = #pass\n",
    "recall = #pass\n",
    "f1 = #pass\n",
    "roc_auc = #pass\n",
    "\n",
    "print(f'Test Accuracy: {accuracy:.4f}')\n",
    "print(f'Test Precision: {precision:.4f}')\n",
    "print(f'Test Recall: {recall:.4f}')\n",
    "print(f'Test F1-Score: {f1:.4f}')\n",
    "print(f'Validation ROC-AUC: {roc_auc:.4f}')\n",
    "\n",
    "#############################################################\n",
    "# END OF YOUR CODE\n",
    "##########################################################\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
