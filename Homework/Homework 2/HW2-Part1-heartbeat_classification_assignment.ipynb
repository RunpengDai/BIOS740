{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "caa89a54",
   "metadata": {},
   "source": [
    "# Jupyter Notebook Assignment: Heartbeat Classification with RNN, LSTM, and GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5d3512",
   "metadata": {},
   "source": [
    "## Title: Heartbeat Classification using Sequence Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637745ea",
   "metadata": {},
   "source": [
    "### Objective:\n",
    "In this assignment, you will build, train, and evaluate three different sequence models (RNN, LSTM, GRU) to classify heartbeats based on ECG data. You will compare their performance using metrics such as AUC, F1-score, and accuracy. Additionally, you will explore the data and visualize the results for better understanding.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1f8096",
   "metadata": {},
   "source": [
    "1. **Dataset Overview:**\n",
    "   - The dataset contains heartbeat signals derived from two well-known datasets: the MIT-BIH Arrhythmia Dataset and PhysioNet's MIT-BIH Arrhythmia Database.\n",
    "   - Each row in the dataset is a time series corresponding to one heartbeat, and the signals represent electrocardiogram (ECG) waveforms.\n",
    "   - The dataset is preprocessed and segmented, with each segment corresponding to a heartbeat. The last column represents the label (5 classes):\n",
    "     - **'N': 0** - Normal heartbeat\n",
    "     - **'S': 1** - Supraventricular premature beat\n",
    "     - **'V': 2** - Premature ventricular contraction\n",
    "     - **'F': 3** - Fusion of ventricular and normal beat\n",
    "     - **'Q': 4** - Unclassifiable beat\n",
    "   - Number of Samples: **109,446**\n",
    "   - Sampling Frequency: **125 Hz**\n",
    "   - Training data: `mitbih_train.csv`\n",
    "   - Testing data: `mitbih_test.csv`\n",
    "   - Data Source: PhysioNet's MIT-BIH Arrhythmia Dataset\n",
    "\n",
    "   **Background:**\n",
    "   - The MIT-BIH Arrhythmia Database includes 48 half-hour excerpts of two-channel ambulatory ECG recordings from 47 subjects, recorded between 1975 and 1979. These recordings were selected to include less common but clinically significant arrhythmias.\n",
    "   - Each recording was digitized at 360 samples per second per channel, annotated independently by two or more cardiologists."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b795b9-12fc-4459-a7e5-1f177ec7b75a",
   "metadata": {},
   "source": [
    "2. **Assignment Tasks:**\n",
    "\n",
    "    * When working on graded functions, please remember to only modify the code that is between the\n",
    "    ```Python\n",
    "    #### START CODE HERE\n",
    "    ```\n",
    "    and\n",
    "    ```Python\n",
    "    #### END CODE HERE\n",
    "    ```\n",
    "\n",
    "   #### Task 1: Data Exploration\n",
    "   - Load the data and explore its structure.\n",
    "   - Plot a few ECG signals for visualization.\n",
    "   - Check the distribution of classes and identify any imbalance.\n",
    "\n",
    "   #### Task 2: Model Implementation\n",
    "   - Implement three sequence models: RNN, LSTM, and GRU.\n",
    "   - Define the architecture for each model.\n",
    "   - Train each model on the training dataset.\n",
    "\n",
    "   #### Task 3: Performance Evaluation\n",
    "   - Evaluate the performance of each model using:\n",
    "     - AUC (Area Under the Curve)\n",
    "     - F1-score\n",
    "     - Accuracy\n",
    "   - Generate a confusion matrix for each model.\n",
    "   - Plot training and testing loss, AUC, F1-score, and accuracy for each epoch.\n",
    "\n",
    "   #### Task 4: Results Comparison\n",
    "   - Compare the performance of the three models using tables and plots.\n",
    "   - Discuss which model performed the best and why.\n",
    "\n",
    "   #### Task 5: Visualization and Insights\n",
    "   - Visualize key insights such as class-wise performance, confusion matrix heatmaps, and metric trends over epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f0a825-5ddf-4028-acb8-48224e26cb84",
   "metadata": {},
   "source": [
    "3. **Deliverables:**\n",
    "   - Submit a completed Jupyter Notebook with:\n",
    "     - Code for all tasks.\n",
    "     - Plots and visualizations.\n",
    "     - A short discussion on the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4bf0f3-eb53-448e-94f0-8ebf718d42c8",
   "metadata": {},
   "source": [
    "4. **Grading Criteria:**\n",
    "   - Correctness and completeness of the implementation.\n",
    "   - Quality of visualizations.\n",
    "   - Depth of analysis in the discussion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddd08b2-1390-4420-991e-d9a41592510b",
   "metadata": {},
   "source": [
    "5. **Tips:**\n",
    "   - Use GPU for faster training.\n",
    "   - Document your code with comments for better readability.\n",
    "   - Use libraries like `matplotlib` and `seaborn` for visualizations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad85c43-a910-4fa4-9f1c-be46fba9b6c7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c809966f",
   "metadata": {},
   "source": [
    "# Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e311bcde",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gdown\n",
    "import gdown\n",
    "# Replace with your actual Google Drive file ID\n",
    "gdown.download(f\"https://drive.google.com/uc?export=download&id=1BFRDzEgPExE9FO1YLFyGoqPJJAXMtXG0\", \"mitbih_train.csv\", quiet=False)\n",
    "gdown.download(f\"https://drive.google.com/uc?export=download&id=127K7q1RVmTT71WQzWMBz612puguD00og\", \"mitbih_test.csv\", quiet=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc38843-7fb2-4d55-8dbc-3422a8474cfe",
   "metadata": {},
   "source": [
    "## Assignments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483af7bd-c64e-429c-8a43-9dc2988bcecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import roc_auc_score, f1_score, accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3108cb-82fb-4bf6-8b48-e83078c09021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "def load_data(train_path, test_path):\n",
    "    \n",
    "    #### START CODE HERE\n",
    "    # Load train and test datasets using pandas\n",
    "    # Note: Each row represents a time series with 187 time steps, each having one feature value, and the last column contains the label indicating 1 of 5 classes\n",
    "    \n",
    "    #### END CODE HERE\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec81664-db9f-4e25-bd9d-2991243d25c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class\n",
    "class HeartbeatDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        \n",
    "        ####START CODE HERE\n",
    "        # Instruction: Replace this with a PyTorch Dataset class implementation.\n",
    "        \n",
    "        #### END CODE HERE\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403cbfa7-88e8-494f-9c09-726cf0596cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data exploration\n",
    "def explore_data(X_train, y_train):\n",
    "    # Check data structure\n",
    "    print(\"Shape of X_train:\", X_train.shape)\n",
    "    print(\"Shape of y_train:\", y_train.shape)\n",
    "\n",
    "    # Plot a few ECG signals\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for i in range(5):\n",
    "        ####START CODE HERE\n",
    "        # Instruction: Replace this with your own implementation to explore and visualize the data.\n",
    "        \n",
    "        #### END CODE HERE\n",
    "    plt.title(\"Sample ECG Signals\")\n",
    "    plt.xlabel(\"Time Step\")\n",
    "    plt.ylabel(\"Amplitude\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Check class distribution\n",
    "    class_counts = pd.Series(y_train).value_counts()\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.barplot(x=class_counts.index, y=class_counts.values, palette=\"viridis\")\n",
    "    plt.title(\"Class Distribution\")\n",
    "    plt.xlabel(\"Class\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4aaef63-9f29-4a7f-aa87-bdbde0fe87fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN Model\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(RNNModel, self).__init__()\n",
    "        ####START CODE HERE\n",
    "        \n",
    "        #### END CODE HERE\n",
    "\n",
    "    def forward(self, x):\n",
    "        ####START CODE HERE\n",
    "        \n",
    "        #### END CODE HERE\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e4e71e-2eba-4aad-bb8f-e7f6afa8bcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM Model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        ####START CODE HERE\n",
    "        \n",
    "        #### END CODE HERE\n",
    "\n",
    "    def forward(self, x):\n",
    "        ####START CODE HERE\n",
    "        \n",
    "        #### END CODE HERE\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297f5b73-e4ee-4b29-a9ef-ff61907a9b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRU Model\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(GRUModel, self).__init__()\n",
    "        ####START CODE HERE\n",
    "        \n",
    "        #### END CODE HERE\n",
    "\n",
    "    def forward(self, x):\n",
    "        ####START CODE HERE\n",
    "        \n",
    "        #### END CODE HERE\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d72401b-01d4-46ec-a519-dd0e7e7cf3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and evaluation functions\n",
    "def train_model(model, criterion, optimizer, train_loader, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    for X, y in train_loader:\n",
    "        ####START CODE HERE\n",
    "        X, y = # TODO\n",
    "        \n",
    "        # TODO\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_targets.extend(y.cpu().numpy())\n",
    "        #### END CODE HERE\n",
    "    \n",
    "    auc = roc_auc_score(pd.get_dummies(all_targets), pd.get_dummies(all_preds), multi_class='ovr')\n",
    "    f1 = f1_score(all_targets, all_preds, average='weighted')\n",
    "    acc = accuracy_score(all_targets, all_preds)\n",
    "    return total_loss / len(train_loader), auc, f1, acc\n",
    "\n",
    "\n",
    "def evaluate_model(model, criterion, data_loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_loader:\n",
    "            ####START CODE HERE\n",
    "            X, y = # TODO\n",
    "\n",
    "            # TODO\n",
    "            \n",
    "            #### END CODE HERE\n",
    "    \n",
    "    auc = roc_auc_score(pd.get_dummies(all_targets), pd.get_dummies(all_preds), multi_class='ovr')\n",
    "    f1 = f1_score(all_targets, all_preds, average='weighted')\n",
    "    acc = accuracy_score(all_targets, all_preds)\n",
    "    return total_loss / len(data_loader), auc, f1, acc\n",
    "\n",
    "def plot_metrics(train_metrics, test_metrics, epochs):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for metric_name in train_metrics.keys():\n",
    "        plt.plot(range(epochs), train_metrics[metric_name], label=f\"Train {metric_name}\")\n",
    "        plt.plot(range(epochs), test_metrics[metric_name], label=f\"Test {metric_name}\", linestyle=\"--\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f7d5ac-308c-418c-8138-929b572e9afb",
   "metadata": {},
   "source": [
    "### Begin analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77564b89-8e21-48a6-afa8-bccd8d06e227",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"mitbih_train.csv\"\n",
    "test_path = \"mitbih_test.csv\"\n",
    "\n",
    "# Load data\n",
    "X_train, y_train, X_test, y_test = load_data(train_path, test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f2cc58-8033-48f2-8567-a1145dade7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore data\n",
    "explore_data(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a67108e-cee8-4083-afcd-e6caec16491c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = HeartbeatDataset(X_train, y_train)\n",
    "test_dataset = HeartbeatDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "####START CODE HERE\n",
    "input_size = # TODO\n",
    "hidden_size = # TODO\n",
    "num_layers = # TODO\n",
    "num_classes = # TODO\n",
    "num_epochs = # TODO\n",
    "device = # TODO\n",
    "#### END CODE HERE\n",
    "\n",
    "models = {\n",
    "    \"RNN\": RNNModel(input_size, hidden_size, num_layers, num_classes).to(device),\n",
    "    \"LSTM\": LSTMModel(input_size, hidden_size, num_layers, num_classes).to(device),\n",
    "    \"GRU\": GRUModel(input_size, hidden_size, num_layers, num_classes).to(device)\n",
    "}\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "results = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Training {model_name}...\")\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    train_metrics = {\"loss\": [], \"auc\": [], \"f1\": [], \"accuracy\": []}\n",
    "    test_metrics = {\"loss\": [], \"auc\": [], \"f1\": [], \"accuracy\": []}\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, train_auc, train_f1, train_acc = train_model(model, criterion, optimizer, train_loader, device)\n",
    "        train_metrics[\"loss\"].append(train_loss)\n",
    "        train_metrics[\"auc\"].append(train_auc)\n",
    "        train_metrics[\"f1\"].append(train_f1)\n",
    "        train_metrics[\"accuracy\"].append(train_acc)\n",
    "\n",
    "        test_loss, test_auc, test_f1, test_acc = evaluate_model(model, criterion, test_loader, device)\n",
    "        test_metrics[\"loss\"].append(test_loss)\n",
    "        test_metrics[\"auc\"].append(test_auc)\n",
    "        test_metrics[\"f1\"].append(test_f1)\n",
    "        test_metrics[\"accuracy\"].append(test_acc)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}: Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}, Test AUC: {test_auc:.4f}, Test F1: {test_f1:.4f}, Test Acc: {test_acc:.4f}\")\n",
    "\n",
    "    results[model_name] = (train_metrics, test_metrics)\n",
    "\n",
    "    # Confusion Matrix\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for X, y in test_loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            outputs = model(X)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            y_true.extend(y.cpu().numpy())\n",
    "            y_pred.extend(preds.cpu().numpy())\n",
    "\n",
    "    ####START CODE HERE\n",
    "    cm = # TODO\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    # TODO\n",
    "    \n",
    "    ### END CODE HERE\n",
    "\n",
    "    plot_metrics(train_metrics, test_metrics, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61de97d5-9249-4771-a860-7d7995c3241b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py10",
   "language": "python",
   "name": "py10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
