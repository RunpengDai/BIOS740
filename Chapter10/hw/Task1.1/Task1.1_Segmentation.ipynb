{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import nibabel as nib\n",
    "import json\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader\n",
    "import torchmetrics\n",
    "\n",
    "from utils.Task1_utils import (\n",
    "    Preprocess_ACDC,\n",
    "    load_config,\n",
    "    ACDC_Dataset,\n",
    "    make_serializeable_metrics,\n",
    "    skin_plot\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task1.1: Build a Vanilla U-Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../img/U-Net.png\" width=\"600\" height=\"400\">\n",
    "\n",
    "Please complete the architecture according to the figure in UNet.\n",
    "\n",
    "**Contracting path (Encoder containing downsampling steps):**\n",
    "\n",
    "Images are first fed through several convolutional layers which reduce height and width, while growing the number of channels.\n",
    "\n",
    "The contracting path follows a regular CNN architecture, with convolutional layers, their activations, and pooling layers to downsample the image and extract its features. In detail, it consists of the repeated application of two 3 x 3 unpadded convolutions, each followed by a rectified linear unit (ReLU) and a 2 x 2 max pooling operation with stride 2 for downsampling. At each downsampling step, the number of feature channels is doubled.\n",
    "\n",
    "Crop function: This step crops the image from the contracting path and concatenates it to the current image on the expanding path to create a skip connection.\n",
    "\n",
    "**Expanding path (Decoder containing upsampling steps):**\n",
    "\n",
    "The expanding path performs the opposite operation of the contracting path, growing the image back to its original size, while shrinking the channels gradually.\n",
    "\n",
    "In detail, each step in the expanding path upsamples the feature map, followed by a 2 x 2 convolution (the transposed convolution). This transposed convolution halves the number of feature channels, while growing the height and width of the image.\n",
    "\n",
    "Next is a concatenation with the correspondingly cropped feature map from the contracting path, and two 3 x 3 convolutions, each followed by a ReLU. You need to perform cropping to handle the loss of border pixels in every convolution.\n",
    "\n",
    "**Final Feature Mapping Block:** \n",
    "\n",
    "In the final layer, a 1x1 convolution is used to map each 64-component feature vector to the desired number of classes. The channel dimensions from the previous layer correspond to the number of filters used, so when you use 1x1 convolutions, you can transform that dimension by choosing an appropriate number of 1x1 filters. When this idea is applied to the last layer, you can reduce the channel dimensions to have one layer per class.\n",
    "\n",
    "> Ronneberger, Olaf, et al. “U-Net: Convolutional Networks for Biomedical Image Segmentation.” arXiv.Org, 18 May 2015, https://arxiv.org/abs/1505.04597v1.\n",
    "\n",
    "The code for block of double convolution is provided.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the seed for reproducibility\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, batch_norm=False):\n",
    "        super().__init__()\n",
    "        if batch_norm:\n",
    "            self.step = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.ReLU(),\n",
    "            )\n",
    "        else:\n",
    "            self.step = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "                nn.ReLU(),\n",
    "            )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.step(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, init_channels=64, batch_norm=False):\n",
    "        super().__init__()\n",
    "\n",
    "        # Hint: Use DoubleConv() module\n",
    "        # Encoder part\n",
    "        # todo\n",
    "\n",
    "        # Add a bottleneck layer (this is the bottleneck)\n",
    "        # todo\n",
    "        \n",
    "        # Decoder part\n",
    "        # todo\n",
    "\n",
    "        # Pooling and upsampling\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2)\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Hint: torch.cat([...], dim=1) to concatenate tensors along channel dimension\n",
    "        # Encoding path\n",
    "        # todo\n",
    "        \n",
    "        # Bottleneck\n",
    "        # todo\n",
    "        \n",
    "        # Decoding path\n",
    "        # todo\n",
    "        \n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare The Dataset\n",
    "\n",
    "Now we will run this U-Net on a public dataset called [Automated Cardiac Diagnosis Challenge(ACDC)](https://www.creatis.insa-lyon.fr/Challenge/acdc/databases.html).\n",
    "\n",
    "\n",
    "The ACDC dataset includes cine MRI scans of the heart in the short-axis view. It provides masked data for the segmentation of various cardiac structures, such as the left ventricle, right ventricle, and myocardium. The dataset consists of 100 subjects for training and 50 subjects for testing, each categorized into five cardiac pathologies and a healthy group. The primary goal is to assess automated methods for cardiac diagnosis.\n",
    "\n",
    "> O. Bernard, A. Lalande, C. Zotti, F. Cervenansky, et al.\n",
    "\"Deep Learning Techniques for Automatic MRI Cardiac Multi-structures Segmentation and\n",
    "Diagnosis: Is the Problem Solved ?\" in IEEE Transactions on Medical Imaging,\n",
    "vol. 37, no. 11, pp. 2514-2525, Nov. 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = \"../data\"\n",
    "\n",
    "# todo: Create a subfolder in `data` folder that should contain 150 subfolders, each corresponding to a patient\n",
    "# todo: You need to put all subjects in `training` and `testing` folder into this same folder.\n",
    "# For example, if you make a folder data/ACDC_all, which has the following structure:\n",
    "# └── patient001\n",
    "# ├── Info.cfg\n",
    "# ├── MANDATORY_CITATION.md\n",
    "# ├── patient001_4d.nii.gz\n",
    "# ├── patient001_frame01_gt.nii.gz\n",
    "# ├── patient001_frame01.nii.gz\n",
    "# ├── patient001_frame12_gt.nii.gz\n",
    "# └── patient001_frame12.nii.gz\n",
    "# Then you should set ACDC_raw_folder = os.path.join(data_folder, \"ACDC_all\")\n",
    "ACDC_raw_folder = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make a plot to get a glimpse of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image1_data = nib.load(os.path.join(ACDC_raw_folder, \"patient002/patient002_frame01.nii.gz\")).get_fdata()\n",
    "mask1_data = nib.load(os.path.join(ACDC_raw_folder, \"patient002/patient002_frame01_gt.nii.gz\")).get_fdata()\n",
    "\n",
    "image1 = image1_data[:, :, 0]\n",
    "mask1 = mask1_data[:, :, 0]\n",
    "mask1 = np.where(mask1 == 0, np.nan, mask1)\n",
    "\n",
    "plt.imshow(image1, cmap='gray')\n",
    "plt.imshow(mask1, cmap='jet', alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The red part is the left ventricle, the blue part is the right ventricle, and the green part is the myocardium."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACDC_raw_folder = os.path.join(data_folder, \"ACDC_all\")\n",
    "\n",
    "len_dict = Preprocess_ACDC(ACDC_raw_folder)\n",
    "print(len_dict)\n",
    "np_data_folder = os.path.join(data_folder, \"np_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now images and corresponding masks are stored in `data/images`, `data/masks`. The npy files are stored in `data/np_data/`, which are the ones we will use next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some parameters we will use later\n",
    "config = load_config(\"../config/Task1.1_config.yaml\")\n",
    "\n",
    "NUM_CLASSES = config[\"dataset\"][\"number_classes\"]\n",
    "INPUT_SIZE = config[\"dataset\"][\"input_size\"]\n",
    "TRAIN_PARAMS = config[\"train\"]\n",
    "MODEL_PARAMS = config[\"model\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing training, validation and testing dataset\n",
    "train_dataset = ACDC_Dataset(\n",
    "    mode=\"train\",\n",
    "    data_dir=np_data_folder,\n",
    "    len_dict=len_dict,\n",
    "    one_hot=True,\n",
    "    num_classes=NUM_CLASSES,\n",
    ")\n",
    "val_dataset = ACDC_Dataset(\n",
    "    mode=\"val\",\n",
    "    data_dir=np_data_folder,\n",
    "    len_dict=len_dict,\n",
    "    one_hot=True,\n",
    "    num_classes=NUM_CLASSES,\n",
    ")\n",
    "test_dataset = ACDC_Dataset(\n",
    "    mode=\"test\",\n",
    "    data_dir=np_data_folder,\n",
    "    len_dict=len_dict,\n",
    "    one_hot=True,\n",
    "    num_classes=NUM_CLASSES,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, **config['data_loader']['train'])\n",
    "val_dataloader = DataLoader(val_dataset, **config['data_loader']['val'])\n",
    "test_dataloader = DataLoader(test_dataset, **config['data_loader']['test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need some metrics to evaluate the performance of our model. Please implement the Dice coefficient and Jaccard index (IoU) using `torchmetrics.MetricCollection`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = torchmetrics.MetricCollection(\n",
    "    [\n",
    "        # todo Implement Dice and JaccardIndex using torchmetrics\n",
    "        # Hint: Don't forget to set ignore_index argument\n",
    "    ],\n",
    "    prefix='metrics/'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Torch device: {device}\")\n",
    "\n",
    "train_metrics = metrics.clone(prefix='train_metrics/').to(device)\n",
    "val_metrics = metrics.clone(prefix='val_metrics/').to(device)\n",
    "test_metrics = metrics.clone(prefix='test_metrics/').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, device, val_dataloader, criterion):\n",
    "    \"\"\"\n",
    "    Function to validate the model. We will use this function in the `train` function.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        val_evaluator = val_metrics.clone().to(device)\n",
    "        val_losses = []\n",
    "        cnt = 0\n",
    "\n",
    "        for idx, batch_data in enumerate(val_dataloader):\n",
    "            images = batch_data['image']\n",
    "            masks = batch_data['mask']            \n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "            preds = model(images)\n",
    "\n",
    "            cnt += images.shape[0]\n",
    "\n",
    "            loss = criterion(preds, masks)\n",
    "            val_losses.append(loss.item())\n",
    "            \n",
    "            # todo: Implement code so that val_evaluator can calculate the metrics\n",
    "            # Hint: It can be done within three lines of code\n",
    "        \n",
    "            if idx == len(val_dataloader) - 1:\n",
    "                # todo: Implement descriptions. \n",
    "                # Hint: Use numpy for loss and train_evaluator for dice and IOU\n",
    "                _loss = f\"curr-loss:{None:0.5f}\"\n",
    "                _dice = f\"dice:{None:0.5f}\"\n",
    "                _iou = f\"iou:{None:0.5f}\"\n",
    "    \n",
    "                print(f\"Validation) -> {_loss}, {_dice}, {_iou}\")\n",
    "        \n",
    "        val_loss = np.sum(val_losses)/cnt\n",
    "    \n",
    "    return val_evaluator, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model,\n",
    "    device,\n",
    "    train_dataloader,\n",
    "    val_dataloader,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    scheduler\n",
    "):\n",
    "    EPOCHS = TRAIN_PARAMS[\"epochs\"]\n",
    "    torch.cuda.empty_cache()\n",
    "    model = model.to(device)\n",
    "\n",
    "    train_evaluator = train_metrics.clone().to(device)\n",
    "    train_iterator = tqdm(total=EPOCHS)\n",
    "\n",
    "    epochs_info = []\n",
    "    best_model = None\n",
    "    best_result = {}\n",
    "    best_val_loss = np.inf\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "\n",
    "        train_evaluator.reset()\n",
    "        train_losses = []\n",
    "        cnt = 0\n",
    "        for idx, batch_data in enumerate(train_dataloader):\n",
    "\n",
    "            images = batch_data[\"image\"]\n",
    "            masks = batch_data[\"mask\"]\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "            preds = model(images)\n",
    "\n",
    "            cnt += images.shape[0]\n",
    "\n",
    "            optimizer.zero_grad()  # clear existing gradients\n",
    "            loss = criterion(preds, masks)\n",
    "            loss.backward()\n",
    "            train_losses.append(loss.item())\n",
    "            optimizer.step()\n",
    "\n",
    "            # todo: Implement code so that train_evaluator can calculate the metrics\n",
    "            # Hint: This should be the same as the case in validate()\n",
    "\n",
    "            if idx == len(train_dataloader) - 1:\n",
    "                # todo: Implement descriptions. Hint: Use numpy for loss and train_evaluator for dice and IOU\n",
    "                # Hint: You only need to modify the prefix of train_metrics\n",
    "                _loss = f\"curr-loss:{None:0.5f}\"\n",
    "                _dice = f\"dice:{None:0.5f}\"\n",
    "                _iou = f\"iou:{None:0.5f}\"\n",
    "\n",
    "                train_iterator.update(1)\n",
    "                print(\n",
    "                    f\"\\nTraining) ep:{epoch+1:03d}/{EPOCHS} -> {_loss}, {_dice}, {_iou}\"\n",
    "                )\n",
    "\n",
    "        train_loss = np.sum(train_losses) / cnt\n",
    "\n",
    "        val_evaluator, val_loss = validate(model, device, val_dataloader, criterion)\n",
    "        if val_loss < best_val_loss:\n",
    "            best_model = model\n",
    "            best_val_loss = val_loss\n",
    "            best_result = {\n",
    "                \"train_loss\": train_loss,\n",
    "                \"val_loss\": val_loss,\n",
    "                \"train_metrics\": make_serializeable_metrics(train_evaluator.compute()),\n",
    "                \"val_metrics\": make_serializeable_metrics(val_evaluator.compute()),\n",
    "            }\n",
    "\n",
    "        epoch_info = {\n",
    "            \"train_loss\": train_loss,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"train_metrics\": make_serializeable_metrics(train_evaluator.compute()),\n",
    "            \"val_metrics\": make_serializeable_metrics(val_evaluator.compute()),\n",
    "        }\n",
    "        epochs_info.append(epoch_info)\n",
    "        train_evaluator.reset()\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "    # Save the info of each epoch and the best result\n",
    "    if os.path.exists(MODEL_PARAMS[\"save_dir\"]):\n",
    "        shutil.rmtree(MODEL_PARAMS[\"save_dir\"], ignore_errors=True)\n",
    "    os.makedirs(MODEL_PARAMS[\"save_dir\"], exist_ok=True)\n",
    "    train_info = {\n",
    "        \"epochs_info\": epochs_info,\n",
    "        \"best_result\": best_result,\n",
    "    }\n",
    "    file_name = \"Train_info.json\"\n",
    "    file_path = os.path.join(MODEL_PARAMS[\"save_dir\"], file_name)\n",
    "    with open(file_path, \"w\") as write_file:\n",
    "        json.dump(train_info, write_file, indent=4)\n",
    "\n",
    "    # save last model's parameters\n",
    "    file_name = \"last_model_state_dict.pt\"\n",
    "    file_path = os.path.join(MODEL_PARAMS[\"save_dir\"], file_name)\n",
    "    torch.save(model.state_dict(), file_path)\n",
    "\n",
    "    # save best model's parameters\n",
    "    file_name = \"best_model_state_dict.pt\"\n",
    "    file_path = os.path.join(MODEL_PARAMS[\"save_dir\"], file_name)\n",
    "    torch.save(best_model.state_dict(), file_path)\n",
    "\n",
    "    return best_model, model, train_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_dataloader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_evaluator = test_metrics.clone().to(device)            \n",
    "        for batch_data in tqdm(test_dataloader):\n",
    "            images = batch_data['image']\n",
    "            masks = batch_data['mask']\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "            \n",
    "            preds = model(images)\n",
    "            \n",
    "            # todo: Implement code so that train_evaluator can calculate the metrics\n",
    "            # Hint: This should be the same as the case in validate()\n",
    "            \n",
    "    return test_evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet(**MODEL_PARAMS[\"params\"])\n",
    "torch.cuda.empty_cache()\n",
    "model = model.to(device)\n",
    "print(\"Number of parameters:\", sum(p.numel() for p in model.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define some essential elements before formally training the U-Net Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Optimizer to Adam with learning rate 0.001\n",
    "optimizer = globals()[TRAIN_PARAMS[\"optimizer\"][\"name\"]](\n",
    "    model.parameters(), lr=TRAIN_PARAMS[\"optimizer\"][\"lr\"]\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, \"min\", **TRAIN_PARAMS[\"scheduler\"]\n",
    ")\n",
    "# During the training, we use cross entropy loss as the criterion\n",
    "def criterion(preds, masks):\n",
    "    # ignore_index should not be set, otherwise background will not be correctly predicted\n",
    "    cross_entropy_loss = CrossEntropyLoss()\n",
    "    return cross_entropy_loss(preds, masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model, last_model, train_info = train(\n",
    "    model,\n",
    "    device,\n",
    "    train_dataloader,\n",
    "    val_dataloader,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    scheduler\n",
    ")\n",
    "print(f\"Best and last models are saved to path {MODEL_PARAMS['save_dir']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = UNet(**MODEL_PARAMS[\"params\"])\n",
    "torch.cuda.empty_cache()\n",
    "best_model = best_model.to(device)\n",
    "\n",
    "file_name = \"best_model_state_dict.pt\"\n",
    "file_path = os.path.join(MODEL_PARAMS[\"save_dir\"], file_name)\n",
    "\n",
    "if device.type == \"cpu\":\n",
    "    best_model.load_state_dict(torch.load(file_path, map_location=torch.device('cpu'), weights_only=True))\n",
    "else:\n",
    "    best_model.load_state_dict(torch.load(file_path, weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_evaluator = test(best_model, device, test_dataloader)\n",
    "print(test_evaluator.compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we make some visualization of the model's performance as well as some sample segmentation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_info_path = f\"{MODEL_PARAMS['save_dir']}/Train_info.json\"\n",
    "with open(train_info_path, \"r\") as f:\n",
    "    train_info = json.loads(\"\".join(f.readlines()))\n",
    "\n",
    "epochs_info = train_info[\"epochs_info\"]\n",
    "train_losses = [d[\"train_loss\"] for d in epochs_info]\n",
    "val_losses = [d[\"val_loss\"] for d in epochs_info]\n",
    "\n",
    "# todo: Implement code to extract train and validation dice and Jaccard index for all epoechs\n",
    "# Hint: Each object should be a list, instead of a single value\n",
    "train_dice = None\n",
    "val_dice = None\n",
    "train_jaccard = None\n",
    "val_jaccard = None\n",
    "\n",
    "\n",
    "_, axs = plt.subplots(1, 3, figsize=[15, 5])\n",
    "\n",
    "axs[0].set_title(\"Loss\")\n",
    "axs[0].plot(train_losses, \"r-\", label=\"train loss\")\n",
    "axs[0].plot(val_losses, \"b-\", label=\"validatiton loss\")\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].set_title(\"Dice score\")\n",
    "axs[1].plot(train_dice, \"r-\", label=\"train dice\")\n",
    "axs[1].plot(val_dice, \"b-\", label=\"validation dice\")\n",
    "axs[1].legend()\n",
    "\n",
    "axs[2].set_title(\"Jaccard Similarity\")\n",
    "axs[2].plot(train_jaccard, \"r-\", label=\"train JaccardIndex\")\n",
    "axs[2].plot(val_jaccard, \"b-\", label=\"validatiton JaccardIndex\")\n",
    "axs[2].legend()\n",
    "\n",
    "plt.savefig(f\"{MODEL_PARAMS['save_dir']}/metrics.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_imgs_dir = f\"{MODEL_PARAMS['save_dir']}/visualized\"\n",
    "\n",
    "if os.path.isdir(save_imgs_dir):\n",
    "    shutil.rmtree(save_imgs_dir)\n",
    "os.mkdir(save_imgs_dir)\n",
    "\n",
    "title_dict = {1: \"RV\", 2: \"Myo\", 3: \"LV\"}  # Right ventricle, myocardium, left ventricle\n",
    "\n",
    "# Visualize one batch of test data\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        ids = batch['id']\n",
    "        images = batch['image']\n",
    "        masks = batch['mask']\n",
    "        images = images.to(device)\n",
    "        preds = best_model(images)\n",
    "\n",
    "        images_np = images.cpu().numpy()\n",
    "        masks_np = masks.cpu().numpy()\n",
    "        preds_np = torch.argmax(preds, 1).cpu().numpy()\n",
    "\n",
    "        for idx in range(len(masks_np)):\n",
    "            for value in range(1, NUM_CLASSES):\n",
    "                if value in np.unique(masks_np[idx]) and value in np.unique(preds_np[idx]):\n",
    "                    image = np.moveaxis(images_np[idx, :3], 0, -1)*255.\n",
    "                    image = np.ascontiguousarray(image, dtype=np.uint8)\n",
    "                    mask = np.where(masks_np[idx] == value, 255, 0)\n",
    "                    mask = np.ascontiguousarray(mask, dtype=np.uint8)\n",
    "                    pred = np.where(preds_np[idx] == value, 255, 0)\n",
    "                    pred = np.ascontiguousarray(pred, dtype=np.uint8)\n",
    "\n",
    "                    plot = skin_plot(image, mask, pred)\n",
    "\n",
    "                    file_id = ids[idx]\n",
    "                    plt.clf()\n",
    "                    plt.title(f\"Comparison of {title_dict[value]} for sample {file_id}\")\n",
    "                    plt.imshow(plot)\n",
    "                    plt.legend()\n",
    "                    plt.savefig(f\"{save_imgs_dir}/Sample_{file_id}_img_{title_dict[value]}.png\")\n",
    "                    plt.close()\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the visualized plots, the red contours represent the predictions and the blue contours represent the ground truth."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
