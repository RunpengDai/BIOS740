{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task2: Exploring Unsupervised Registration using Voxelmorph\n",
    "\n",
    "Medical image registration has been a cornerstone in the research fields of medical image computing and computer assisted intervention, responsible for many clinical applications. Whilst machine learning methods have long been important in developing pairwise algorithms, recently proposed deep-learning-based frameworks directly infer displacement fields without iterative optimisation for unseen image pairs, using neural networks trained from large population data. These novel approaches promise to tackle several most challenging aspects previously faced by classical pairwise methods, such as high computational cost, robustness for generalisation and lack of inter-modality similarity measures. \n",
    "\n",
    "Note the package makes use of  `Neurite`, a neural networks toolbox that is based on Tensorflow rather than Pytorch. **Due to compatibility issues, you must make the following changes:**\n",
    "\n",
    "1. Follow https://github.com/tensorflow/tensorflow/issues/70796 so that `import voxelmorph` can be run successfully.\n",
    "2. Replace all `get_shape()` with `shape` in the `<voxelmorph package>/tf/networks.py` ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Registration1: MNIST Dataset\n",
    "\n",
    "### Loading the Dataset\n",
    "\n",
    "We will learn the preliminearies about (unsupervised) DL image registration through the simple MNIST dataset and the popular *VoxelMorph* framework. VoxelMorph is a fast learning-based framework for deformable, pairwise medical image registration. Instead of optimizing an objective function for each pair of images which can be time-consuming, it formulates registration as a function that maps an input image to a deformation field that aligns these images. Given a new pair of scans, VoxelMorph rapidly computes a deformation field by directly evaluating the function.\n",
    "\n",
    "> Balakrishnan, Guha, et al. “VoxelMorph: A Learning Framework for Deformable Medical Image Registration.” IEEE Transactions on Medical Imaging, vol. 38, no. 8, Aug. 2019, pp. 1788–800. DOI.org (Crossref), https://doi.org/10.1109/TMI.2019.2897538.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "os.environ['TF_USE_LEGACY_KERAS'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neurite as ne\n",
    "import voxelmorph as vxm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "\n",
    "train_loader = datasets.MNIST('../data', train=True, download=True)\n",
    "test_loader = datasets.MNIST('../data', train=False, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array([np.array(image) for image, _ in train_loader])\n",
    "y_train = np.array([label for _, label in train_loader], dtype=object)\n",
    "x_test = np.array([np.array(image) for image, _ in test_loader])\n",
    "y_test = np.array([label for _, label in test_loader], dtype=object)\n",
    "\n",
    "# Create validation set as well\n",
    "x_val = x_train[-10000:]\n",
    "y_val = y_train[-10000:]\n",
    "x_train = x_train[:-10000]\n",
    "y_train = y_train[:-10000]\n",
    "\n",
    "# let's get some shapes to understand what we loaded.\n",
    "print('shape of x_train: ', x_train.shape)\n",
    "print('shape of y_train: ', y_train.shape)\n",
    "print('shape of x_val: ', x_val.shape)\n",
    "print('shape of y_val: ', y_val.shape)\n",
    "print('shape of x_test: ', x_test.shape)\n",
    "print('shape of y_test: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We select first 5 images from the training set and visualize them\n",
    "\n",
    "fig, ax = plt.subplots(1, 5, figsize=(15, 5))\n",
    "for i in range(5):\n",
    "    ax[i].imshow(x_train[i], cmap='gray')\n",
    "    ax[i].axis('off')\n",
    "    ax[i].set_title('Label: ' + str(y_train[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the last step of loading, we pre-process the data to make it suitable for the VoxelMorph framework.  For demonstration, we will create a stand alone dataset that **contains only number 4, 5, 7**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train/255\n",
    "x_val = x_val/255\n",
    "x_test = x_test/255\n",
    "\n",
    "# Change size from 28*28 to 32*32\n",
    "pad_amount = ((0, 0), (2,2), (2,2))\n",
    "x_train = np.pad(x_train, pad_amount, 'constant') # pad with constant values\n",
    "x_val = np.pad(x_val, pad_amount, 'constant')\n",
    "x_test = np.pad(x_test, pad_amount, 'constant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_4 = x_train[y_train == 4]\n",
    "x_val_4 = x_val[y_val == 4]\n",
    "x_test_4 = x_test[y_test == 4]\n",
    "\n",
    "x_train_5 = x_train[y_train == 5]\n",
    "x_val_5 = x_val[y_val == 5]\n",
    "x_test_5 = x_test[y_test == 5]\n",
    "\n",
    "x_train_7 = x_train[y_train == 7]\n",
    "x_val_7 = x_val[y_val == 7]\n",
    "x_test_7 = x_test[y_test == 7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize again\n",
    "fig, ax = plt.subplots(1, 5, figsize=(15, 5))\n",
    "for i in range(5):\n",
    "    ax[i].imshow(x_train_7[i], cmap='gray')\n",
    "    ax[i].axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Neural Networks used to Estimate Deformation\n",
    "\n",
    "In learning-based methods, we use a neural network that takes in two images $m$ and $f$ (e.g. MNIST digits of size 32x32) and outputs a dense deformation $\\phi$ (e.g. size 32x32x2, because at each pixel we want a vector telling us where to go). In this case, we abstract the UNet model from the VoxelMorph.\n",
    "\n",
    "Note that we only consider non-rigid deformation here as there are lots of handy tools to conduct affine registrations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_DIM = 2\n",
    "INPUT_FEATURES = 2\n",
    "\n",
    "input_shape = (*x_train.shape[1:], INPUT_FEATURES)\n",
    "print('input shape: ', input_shape)\n",
    "\n",
    "n_encoder_features = [32, 32, 32, 32]\n",
    "n_decoder_features = [32, 32, 32, 32, 32, 16]\n",
    "n_features = [n_encoder_features, n_decoder_features]\n",
    "\n",
    "# inshape: Input tensor shape\n",
    "# nb_features: UNet convolutional features, specified via a list of lists of the form [[encoder feats], [decoder feats]]\n",
    "UNet = vxm.networks.Unet(inshape=input_shape, nb_features=n_features)\n",
    "\n",
    "print(f\"Shape of input: {UNet.input[0].shape}\")\n",
    "print(f\"Shape of output: {UNet.output[0].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use the UNet to replicate the structure presented in VoxelMorph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deformation Model: UNet + Conv2D\n",
    "class DeformationModel(nn.Module):\n",
    "    def __init__(self, UNet, N_DIM):\n",
    "        super(DeformationModel, self).__init__()\n",
    "        self.unet = UNet\n",
    "        self.conv = nn.Conv2d(in_channels=UNet.output, out_channels=N_DIM, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.UNet(x)\n",
    "        deformation_tensor = self.conv(x)\n",
    "        return deformation_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full Model: UNet + Conv2D + Spatial Transformer\n",
    "class VoxelMorphModel(nn.Module):\n",
    "    def __init__(self, UNet, N_DIM):\n",
    "        super(VoxelMorphModel, self).__init__()\n",
    "        self.deformation_model = DeformationModel(UNet, N_DIM)\n",
    "        self.spatial_transformer = vxm.layers.SpatialTransformer(name='transformer')\n",
    "\n",
    "    def forward(self, x):\n",
    "        deformation_tensor = self.deformation_model(x)\n",
    "        moved_image_tensor = self.spatial_transformer([x, deformation_tensor])\n",
    "        out_tensor = [moved_image_tensor, deformation_tensor]\n",
    "        return out_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model we just created represents the standard, dense VoxelMorph archetecture, with **a UNet component, displacement field, and final spatial transformer layer**.\n",
    "\n",
    "<img src=\"../img/VoxelMorph.png\" width=\"800\" height=\"400\">\n",
    "\n",
    "Actually, it is not necessary for us to repeat such code every time. Voxelmorph has a built-in `VxmDense` class to define such models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We no longer need to supply 2 in input_shape, as the model will automatically configured with two input tensorw (moving and fixed inputs) instead of one\n",
    "vxm_model = vxm.networks.VxmDense(inshape=input_shape[:2], nb_unet_features=n_features, int_steps=0) # Here int_steps=0 option disables diffeomorphism\n",
    "print(f\"Shape of input: {[str(t.shape) for t in vxm_model.inputs]}\")\n",
    "print(f\"Shape of output: {[str(t.shape) for t in vxm_model.outputs]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Loss Function\n",
    "\n",
    "In unsupervised registration where no gold standard is provided, how could we know whether the deformation is good or not?\n",
    "\n",
    "1. First make sure $m\\circ \\phi$, that is, the moving image warped by the deformation $\\phi$ to be close to the fixed image $f$.\n",
    "2. Then regularize $\\phi$, ensuring that the deformation is smooth enough without eccentric behavior.\n",
    "\n",
    "Let us define two losses: a simple MSE and a spatial gradient of the displacement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# voxelmorph has a variety of custom loss classes\n",
    "losses = [vxm.losses.MSE().loss, vxm.losses.Grad('l2').loss]\n",
    "lambda_param = 0.05\n",
    "loss_weights = [1, lambda_param]  # balance the two losses through hyperparameter\n",
    "\n",
    "vxm_model.compile(optimizer='Adam', loss=losses, loss_weights=loss_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model\n",
    "\n",
    "After making an instance of model, we need to make sure the data is in the right format through a python generator that gives batches of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vxm_data_generator(x, batch_size=32):\n",
    "    \"\"\"\n",
    "    Generator that takes in data of size [N, H, W], and yields data for\n",
    "    our custom vxm model. Note that we need to provide numpy data for each\n",
    "    input, and each output.\n",
    "\n",
    "    inputs:  moving [bs, H, W, 1], fixed image [bs, H, W, 1]\n",
    "    outputs: moved image [bs, H, W, 1], zero-gradient [bs, H, W, 2]\n",
    "    \"\"\"\n",
    "\n",
    "    # preliminary sizing\n",
    "    volume_shape = x.shape[1:] # extract data shape\n",
    "    n_dim = len(volume_shape)\n",
    "    \n",
    "    # prepare a zero array the size of the deformation, explained later\n",
    "    zero_phi = np.zeros([batch_size, *volume_shape, n_dim])\n",
    "    \n",
    "    while True:\n",
    "        # randomly select pairs of images for training\n",
    "        idx1 = np.random.randint(0, x.shape[0], size=batch_size)\n",
    "        moving_images = x[idx1, ..., np.newaxis]\n",
    "        idx2 = np.random.randint(0, x.shape[0], size=batch_size)\n",
    "        fixed_images = x[idx2, ..., np.newaxis]\n",
    "\n",
    "        inputs = [moving_images, fixed_images]\n",
    "        outputs = [fixed_images, zero_phi]\n",
    "        \n",
    "        yield (inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make use of the generator\n",
    "train_generator = vxm_data_generator(x_train_7)\n",
    "in_sample, out_sample = next(train_generator)\n",
    "\n",
    "print(in_sample[0].shape)\n",
    "images = [img[0, :, :, 0] for img in in_sample + out_sample] \n",
    "titles = ['Moving', 'Fixed', 'Moved ground-truth (Fixed)', 'Zero Deformation Field']\n",
    "fig, ax = plt.subplots(1, 4, figsize=(15, 5))\n",
    "for i in range(4):\n",
    "    ax[i].imshow(images[i], cmap='gray')\n",
    "    ax[i].axis('off')\n",
    "    ax[i].set_title(titles[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "steps_per_epoch = 100\n",
    "history = vxm_model.fit_generator(train_generator, epochs=epochs, steps_per_epoch=steps_per_epoch, verbose=2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the loss\n",
    "plt.figure()\n",
    "plt.plot(history.epoch, history.history['loss'], '.-')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Registration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the trained model using validation set\n",
    "val_generator = vxm_data_generator(x_val_7, batch_size = 1)\n",
    "val_input, _ = next(val_generator)\n",
    "val_pred = vxm_model.predict(val_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the prediction\n",
    "images = [img[0, :, :, 0] for img in val_input + val_pred] \n",
    "titles = ['Moving', 'Fixed', 'Moved', 'Flow']\n",
    "fig, ax = plt.subplots(1, 4, figsize=(15, 5))\n",
    "for i in range(4):\n",
    "    ax[i].imshow(images[i], cmap='gray')\n",
    "    ax[i].axis('off')\n",
    "    ax[i].set_title(titles[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The visualization of flow field might be a bit tricky. The `neurite` package provides a handy function `plot.flow` to visualize the displacement field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ne.plot.flow([val_pred[1].squeeze()], width=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you might wonder, can we use the learned model for other numbers such as 4 or 5? Let us make some experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_generator = vxm_data_generator(x_val_4, batch_size = 1)\n",
    "val_input, _ = next(val_generator)\n",
    "val_pred = vxm_model.predict(val_input)\n",
    "\n",
    "# visualize the prediction\n",
    "images = [img[0, :, :, 0] for img in val_input + val_pred]\n",
    "titles = ['Moving', 'Fixed', 'Moved', 'Flow']\n",
    "fig, ax = plt.subplots(1, 4, figsize=(15, 5))\n",
    "for i in range(4):\n",
    "    ax[i].imshow(images[i], cmap='gray')\n",
    "    ax[i].axis('off')\n",
    "    ax[i].set_title(titles[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_generator = vxm_data_generator(x_val_5, batch_size = 1)\n",
    "val_input, _ = next(val_generator)\n",
    "val_pred = vxm_model.predict(val_input)\n",
    "\n",
    "# visualize the prediction\n",
    "images = [img[0, :, :, 0] for img in val_input + val_pred]\n",
    "titles = ['Moving', 'Fixed', 'Moved', 'Flow']\n",
    "fig, ax = plt.subplots(1, 4, figsize=(15, 5))\n",
    "for i in range(4):\n",
    "    ax[i].imshow(images[i], cmap='gray')\n",
    "    ax[i].axis('off')\n",
    "    ax[i].set_title(titles[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting - it still works with a bit worse performance, meaning that it generalizes beyond what we expected. Why? Locally, parts of the 7s look similar to the 4s as well as 5s, so the registration algorithm still tries to match local neighborhoods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Registration2: Brain MRI\n",
    "\n",
    "Cleary MNIST is just a toy dataset. Let us try a more realistic example - brain MRI images. You can find them in `data/Brain`.\n",
    "\n",
    "These brain images have gone through intensity-normalized affinely alignment, and are skull-stripped with FreeSurfer, so that we can concentrate on conducting deformable registration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brain_data = np.load('../data/Brain/tutorial_data.npz')\n",
    "x_train = brain_data['train']\n",
    "x_val = brain_data['validate']\n",
    "\n",
    "# There are 208 volumes, each of size 160x192\n",
    "volume_shape = x_train.shape[1:]\n",
    "print('Shape of x_train:', x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make some visualizations for first 5 volumes\n",
    "fig, ax = plt.subplots(1, 5, figsize=(20, 5))\n",
    "for i in range(5):\n",
    "    ax[i].imshow(x_train[i, ...], cmap = 'gray')\n",
    "    ax[i].axis('off')\n",
    "    cbar = plt.colorbar(ax[i].imshow(x_train[i, ...], cmap='gray'), ax=ax[i])\n",
    "    cbar.ax.tick_params(labelsize=8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will repeat the same setting as above for the MNIST dataset, with the tunning parameter adjusted to 0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the VoxelMorph model\n",
    "# todo\n",
    "vxm_model = vxm.networks.VxmDense(volume_shape, n_features, int_steps=0)\n",
    "\n",
    "losses = [vxm.losses.MSE().loss, vxm.losses.Grad('l2').loss]\n",
    "lambda_param = 0.01\n",
    "loss_weights = [1, lambda_param]\n",
    "\n",
    "vxm_model.compile(optimizer='Adam', loss=losses, loss_weights=loss_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data generator\n",
    "# todo\n",
    "train_generator = None\n",
    "in_sample, out_sample = None\n",
    "\n",
    "# visualize\n",
    "# todo\n",
    "# Hint: Follow the same procedure as in the MNIST example\n",
    "images = None\n",
    "titles = None\n",
    "fig, ax = plt.subplots(1, 4, figsize=(15, 5))\n",
    "for i in range(4):\n",
    "    ax[i].imshow(images[i], cmap='gray')\n",
    "    ax[i].axis('off')\n",
    "    ax[i].set_title(titles[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the brains are much more complex than the MNIST digits, the `epochs` should be increased to 200. We adjust `steps_per_epoch` to $\\lceil 208/8\\rceil=26$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 200\n",
    "steps_per_epoch = 26\n",
    "# todo\n",
    "history = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the loss\n",
    "plt.figure()\n",
    "plt.plot(history.epoch, history.history['loss'], '.-')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo\n",
    "val_generator = None\n",
    "val_input, _ = None\n",
    "\n",
    "val_pred = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize both the images and the flow as before\n",
    "# todo\n",
    "images = None\n",
    "titles = None\n",
    "fig, ax = plt.subplots(1, 4, figsize=(15, 5))\n",
    "for i in range(4):\n",
    "    ax[i].imshow(images[i], cmap='gray')\n",
    "    ax[i].axis('off')\n",
    "    ax[i].set_title(titles[i])\n",
    "\n",
    "\n",
    "flow = val_pred[1].squeeze()[::3,::3]\n",
    "ne.plot.flow([flow], width=5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first look, the MSE-only model matches the fixed image better. However, we can observe that the obtained deformation field is very noisy and unlikely to be anatomically meaningful. We might need to make use of anatomical segmentations for evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concluding Remarks\n",
    "\n",
    "Here are some tutorials if you want to dive deep into how to use deep learning for medical image registration:\n",
    "\n",
    "+ Visualize warp: https://colab.research.google.com/drive/1F8f1imh5WfyBv-crllfeJBFY16-KHl9c?usp=sharing\n",
    "+ Template(Atlas) construction: https://colab.research.google.com/drive/1SkQbrWTQHpQFrG4J2WoBgGZC9yAzUas2?usp=sharing#scrollTo=ADanmU8xde-N\n",
    "\n",
    "Evaluating registration is tricky. For more details, here is a useful reference:\n",
    "\n",
    "> Song, Joo Hyun. Methods for Evaluating Image Registration. 2017. University of Iowa, Doctor of Philosophy. DOI.org (Crossref), https://doi.org/10.17077/etd.v0vailob."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
